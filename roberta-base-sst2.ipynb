{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7cf164-2fe2-4061-81b4-fbdfc2f34f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "from IPython.display import clear_output\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d70c48-1c67-4213-a98d-3e7c7ac26e0b",
   "metadata": {},
   "source": [
    "# LoRA Setup\n",
    "\n",
    "First, we need to setup a LoRALayer class which we can then inject into our transformer model, switching out only the query and value attention layers as stated in the original paper. The following code should work on any transformer model and therefore is model-agnostic, as long as the query and value layers hold \"query\" and \"value\" in their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ff64fc-a4f3-4c02-939c-d9e54cb695b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4, alpha=1):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.randn(original_layer.in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, original_layer.out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_output = self.original_layer(x)\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        return original_output + lora_output\n",
    "\n",
    "def apply_lora(model, rank=4, alpha=1):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(x in name for x in ['query', 'value']):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            child_name = name.split('.')[-1]\n",
    "            parent = model.get_submodule(parent_name)\n",
    "            lora_layer = LoRALayer(module, rank, alpha)\n",
    "            setattr(parent, child_name, lora_layer)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_A' in name or 'lora_B' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b92aec5-e55d-4561-869a-4863c67a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (predictions == batch['labels']).sum().item()\n",
    "        total_samples += batch['labels'].size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_loss = total_loss / (batch_idx + 1)\n",
    "            current_accuracy = total_correct / total_samples\n",
    "            print(f\"  Batch {batch_idx+1}: Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.4f}\")\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy, epoch_time\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total_samples += batch[\"labels\"].size(0)\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy, eval_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed85a32-a347-4c16-af9a-c85432dd2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, eval_dataloader, learning_rate, num_epochs, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=250, num_training_steps=len(train_dataloader) * num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_losses = []\n",
    "    eval_accuracies = []\n",
    "    \n",
    "    total_train_time = 0\n",
    "    total_eval_time = 0\n",
    "    peak_memory_usage = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_accuracy, train_time = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        total_train_time += train_time\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Update peak memory usage\n",
    "        current_memory = torch.cuda.memory_allocated() / 1e9  # Convert to GB\n",
    "        peak_memory_usage = max(peak_memory_usage, current_memory)\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_loss, eval_accuracy, eval_time = evaluate(model, eval_dataloader, device)\n",
    "        total_eval_time += eval_time\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Time: {train_time:.2f}s\")\n",
    "        print(f\"  Eval Loss: {eval_loss:.4f}, Accuracy: {eval_accuracy:.4f}, Time: {eval_time:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'eval_losses': eval_losses,\n",
    "        'eval_accuracies': eval_accuracies,\n",
    "        'total_train_time': total_train_time,\n",
    "        'total_eval_time': total_eval_time,\n",
    "        'peak_memory_usage': peak_memory_usage,\n",
    "        'final_train_accuracy': train_accuracies[-1],\n",
    "        'final_eval_accuracy': eval_accuracies[-1]\n",
    "    }\n",
    "\n",
    "def print_train_results(name, results):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total training time: {results['total_train_time']:.2f}s\")\n",
    "    print(f\"  Total evaluation time: {results['total_eval_time']:.2f}s\")\n",
    "    print(f\"  Peak GPU memory usage: {results['peak_memory_usage']:.2f}GB\")\n",
    "    print(f\"  Final evaluation accuracy: {results['eval_accuracies'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71248bfc-8fc3-4482-b16c-6076eaf4b792",
   "metadata": {},
   "source": [
    "# Reproducing GLUE with RoBeRTa-base\n",
    "\n",
    "One of the central experiments of the paper that we want to reproduce is the GLUE benchmark on several models with different finetuning strategies. We restrict ourselves to comparing full fine-tuning with LoRA on RoBeRTa-base, testing SST-2, XXX and XXX of the GLUE benchmark. Because we do not have the resources to create a proper submission for GLUE and therefore also cannot do a proper evaluation on the test split, we create our own test split from part of the train split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363344a-92a0-4939-a25f-d41f69954971",
   "metadata": {},
   "source": [
    "## SST-2\n",
    "\n",
    "The first task we reproduce is the Stanford Sentiment Treebank (SST-2). It contains sentences and phrases annotated for sentiment analysis, therefore being a binary classification task. As in the paper, we will use a batch size of 16, a learning rate of 5e-4 for LoRA, a rank of 8, alpha of 8 and a maximum sequence length of 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1becc97-2a51-4472-81e4-bcf9851be6cf",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7c88ec-f792-4671-b1ee-4a9250f09e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "full_learning_rate = 1e-5\n",
    "lora_learning_rate = 5e-4\n",
    "rank = 8\n",
    "alpha = 8\n",
    "max_sequence_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc7755a-73c5-4bf7-8edd-ab15885b1a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa5ac982a84d58b26788cc9683df50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddea75b584e44afcaff7dd445ff6cbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd775729b70452191e92887e879a6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_sequence_length)\n",
    "\n",
    "split = raw_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "dataset = split\n",
    "dataset[\"validation\"] = raw_dataset[\"validation\"]\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e26826-112f-4860-b4d4-ec36b6c2dcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine-tuning trainable parameters: 124647170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA trainable parameters: 294912\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Full fine-tuning setup\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\n",
    "full_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)\n",
    "print(f\"Full fine-tuning trainable parameters: {full_params}\")\n",
    "\n",
    "# LoRA setup\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "lora_model = apply_lora(lora_model, rank=rank, alpha=alpha).to(device)\n",
    "lora_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA trainable parameters: {lora_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f9c9e4-e617-4e9b-99ff-a01ad2fe9e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): LoRALayer(\n",
       "                (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LoRALayer(\n",
       "                (original_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a01d00-fe1c-4e38-abb3-9bf8549b19b8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4533ce8c-b45d-4b22-a42c-7b30d9fda7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full fine-tuning experiment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf1239d1a13403f813d1a1c2cf010c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.6894, Accuracy: 0.5463\n",
      "  Batch 200: Loss: 0.6287, Accuracy: 0.6122\n",
      "  Batch 300: Loss: 0.5339, Accuracy: 0.6933\n",
      "  Batch 400: Loss: 0.4724, Accuracy: 0.7406\n",
      "  Batch 500: Loss: 0.4359, Accuracy: 0.7672\n",
      "  Batch 600: Loss: 0.4062, Accuracy: 0.7884\n",
      "  Batch 700: Loss: 0.3873, Accuracy: 0.8035\n",
      "  Batch 800: Loss: 0.3703, Accuracy: 0.8152\n",
      "  Batch 900: Loss: 0.3563, Accuracy: 0.8239\n",
      "  Batch 1000: Loss: 0.3464, Accuracy: 0.8311\n",
      "  Batch 1100: Loss: 0.3346, Accuracy: 0.8386\n",
      "  Batch 1200: Loss: 0.3243, Accuracy: 0.8449\n",
      "  Batch 1300: Loss: 0.3162, Accuracy: 0.8501\n",
      "  Batch 1400: Loss: 0.3096, Accuracy: 0.8549\n",
      "  Batch 1500: Loss: 0.3037, Accuracy: 0.8587\n",
      "  Batch 1600: Loss: 0.2997, Accuracy: 0.8609\n",
      "  Batch 1700: Loss: 0.2938, Accuracy: 0.8641\n",
      "  Batch 1800: Loss: 0.2900, Accuracy: 0.8671\n",
      "  Batch 1900: Loss: 0.2863, Accuracy: 0.8693\n",
      "  Batch 2000: Loss: 0.2804, Accuracy: 0.8728\n",
      "  Batch 2100: Loss: 0.2764, Accuracy: 0.8752\n",
      "  Batch 2200: Loss: 0.2727, Accuracy: 0.8771\n",
      "  Batch 2300: Loss: 0.2690, Accuracy: 0.8793\n",
      "  Batch 2400: Loss: 0.2653, Accuracy: 0.8813\n",
      "  Batch 2500: Loss: 0.2628, Accuracy: 0.8827\n",
      "  Batch 2600: Loss: 0.2607, Accuracy: 0.8840\n",
      "  Batch 2700: Loss: 0.2583, Accuracy: 0.8856\n",
      "  Batch 2800: Loss: 0.2560, Accuracy: 0.8870\n",
      "  Batch 2900: Loss: 0.2540, Accuracy: 0.8885\n",
      "  Batch 3000: Loss: 0.2515, Accuracy: 0.8899\n",
      "  Batch 3100: Loss: 0.2499, Accuracy: 0.8911\n",
      "  Batch 3200: Loss: 0.2488, Accuracy: 0.8919\n",
      "  Batch 3300: Loss: 0.2470, Accuracy: 0.8930\n",
      "  Batch 3400: Loss: 0.2449, Accuracy: 0.8942\n",
      "  Batch 3500: Loss: 0.2435, Accuracy: 0.8950\n",
      "  Batch 3600: Loss: 0.2419, Accuracy: 0.8959\n",
      "  Batch 3700: Loss: 0.2408, Accuracy: 0.8965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec2c3df83944e959f556dcd183de941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.2393, Accuracy: 0.8973, Time: 5293.65s\n",
      "  Eval Loss: 0.1840, Accuracy: 0.9461, Time: 25.52s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d309e1d76e04c038cee29db7a00aec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1362, Accuracy: 0.9450\n",
      "  Batch 200: Loss: 0.1377, Accuracy: 0.9472\n",
      "  Batch 300: Loss: 0.1371, Accuracy: 0.9475\n",
      "  Batch 400: Loss: 0.1377, Accuracy: 0.9469\n",
      "  Batch 500: Loss: 0.1368, Accuracy: 0.9475\n",
      "  Batch 600: Loss: 0.1377, Accuracy: 0.9475\n",
      "  Batch 700: Loss: 0.1414, Accuracy: 0.9464\n",
      "  Batch 800: Loss: 0.1414, Accuracy: 0.9459\n",
      "  Batch 900: Loss: 0.1410, Accuracy: 0.9465\n",
      "  Batch 1000: Loss: 0.1416, Accuracy: 0.9464\n",
      "  Batch 1100: Loss: 0.1416, Accuracy: 0.9470\n",
      "  Batch 1200: Loss: 0.1422, Accuracy: 0.9466\n",
      "  Batch 1300: Loss: 0.1409, Accuracy: 0.9471\n",
      "  Batch 1400: Loss: 0.1414, Accuracy: 0.9468\n",
      "  Batch 1500: Loss: 0.1410, Accuracy: 0.9466\n",
      "  Batch 1600: Loss: 0.1402, Accuracy: 0.9472\n",
      "  Batch 1700: Loss: 0.1401, Accuracy: 0.9474\n",
      "  Batch 1800: Loss: 0.1401, Accuracy: 0.9474\n",
      "  Batch 1900: Loss: 0.1411, Accuracy: 0.9474\n",
      "  Batch 2000: Loss: 0.1404, Accuracy: 0.9477\n",
      "  Batch 2100: Loss: 0.1400, Accuracy: 0.9478\n",
      "  Batch 2200: Loss: 0.1393, Accuracy: 0.9480\n",
      "  Batch 2300: Loss: 0.1401, Accuracy: 0.9475\n",
      "  Batch 2400: Loss: 0.1403, Accuracy: 0.9474\n",
      "  Batch 2500: Loss: 0.1396, Accuracy: 0.9477\n",
      "  Batch 2600: Loss: 0.1391, Accuracy: 0.9479\n",
      "  Batch 2700: Loss: 0.1394, Accuracy: 0.9477\n",
      "  Batch 2800: Loss: 0.1391, Accuracy: 0.9478\n",
      "  Batch 2900: Loss: 0.1392, Accuracy: 0.9477\n",
      "  Batch 3000: Loss: 0.1390, Accuracy: 0.9477\n",
      "  Batch 3100: Loss: 0.1394, Accuracy: 0.9475\n",
      "  Batch 3200: Loss: 0.1401, Accuracy: 0.9472\n",
      "  Batch 3300: Loss: 0.1399, Accuracy: 0.9473\n",
      "  Batch 3400: Loss: 0.1395, Accuracy: 0.9475\n",
      "  Batch 3500: Loss: 0.1392, Accuracy: 0.9476\n",
      "  Batch 3600: Loss: 0.1390, Accuracy: 0.9477\n",
      "  Batch 3700: Loss: 0.1389, Accuracy: 0.9478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd580f1dd91f47f4a8c803159332d394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.1389, Accuracy: 0.9477, Time: 5313.38s\n",
      "  Eval Loss: 0.1801, Accuracy: 0.9369, Time: 25.88s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bc8e83c4e541c8ba4bfb296b28bfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.0820, Accuracy: 0.9712\n",
      "  Batch 200: Loss: 0.0875, Accuracy: 0.9691\n",
      "  Batch 300: Loss: 0.0920, Accuracy: 0.9665\n",
      "  Batch 400: Loss: 0.0941, Accuracy: 0.9663\n",
      "  Batch 500: Loss: 0.0975, Accuracy: 0.9651\n",
      "  Batch 600: Loss: 0.0999, Accuracy: 0.9643\n",
      "  Batch 700: Loss: 0.1008, Accuracy: 0.9637\n",
      "  Batch 800: Loss: 0.0991, Accuracy: 0.9648\n",
      "  Batch 900: Loss: 0.0993, Accuracy: 0.9644\n",
      "  Batch 1000: Loss: 0.1008, Accuracy: 0.9639\n",
      "  Batch 1100: Loss: 0.1016, Accuracy: 0.9636\n",
      "  Batch 1200: Loss: 0.1026, Accuracy: 0.9635\n",
      "  Batch 1300: Loss: 0.1020, Accuracy: 0.9638\n",
      "  Batch 1400: Loss: 0.1009, Accuracy: 0.9642\n",
      "  Batch 1500: Loss: 0.1007, Accuracy: 0.9642\n",
      "  Batch 1600: Loss: 0.1008, Accuracy: 0.9643\n",
      "  Batch 1700: Loss: 0.1002, Accuracy: 0.9643\n",
      "  Batch 1800: Loss: 0.1007, Accuracy: 0.9639\n",
      "  Batch 1900: Loss: 0.1009, Accuracy: 0.9637\n",
      "  Batch 2000: Loss: 0.1020, Accuracy: 0.9637\n",
      "  Batch 2100: Loss: 0.1015, Accuracy: 0.9639\n",
      "  Batch 2200: Loss: 0.1017, Accuracy: 0.9636\n",
      "  Batch 2300: Loss: 0.1014, Accuracy: 0.9638\n",
      "  Batch 2400: Loss: 0.1017, Accuracy: 0.9636\n",
      "  Batch 2500: Loss: 0.1009, Accuracy: 0.9637\n",
      "  Batch 2600: Loss: 0.1008, Accuracy: 0.9637\n",
      "  Batch 2700: Loss: 0.1009, Accuracy: 0.9637\n",
      "  Batch 2800: Loss: 0.1010, Accuracy: 0.9638\n",
      "  Batch 2900: Loss: 0.1013, Accuracy: 0.9636\n",
      "  Batch 3000: Loss: 0.1012, Accuracy: 0.9636\n",
      "  Batch 3100: Loss: 0.1010, Accuracy: 0.9638\n",
      "  Batch 3200: Loss: 0.1009, Accuracy: 0.9638\n",
      "  Batch 3300: Loss: 0.0999, Accuracy: 0.9641\n",
      "  Batch 3400: Loss: 0.1003, Accuracy: 0.9640\n",
      "  Batch 3500: Loss: 0.1002, Accuracy: 0.9639\n",
      "  Batch 3600: Loss: 0.1002, Accuracy: 0.9639\n",
      "  Batch 3700: Loss: 0.0999, Accuracy: 0.9640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4c0e5e15d641b1831d5108954f5940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.0997, Accuracy: 0.9640, Time: 5316.09s\n",
      "  Eval Loss: 0.2085, Accuracy: 0.9438, Time: 25.91s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f40e00004c4189a383580287ded68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.0666, Accuracy: 0.9762\n",
      "  Batch 200: Loss: 0.0654, Accuracy: 0.9747\n",
      "  Batch 300: Loss: 0.0659, Accuracy: 0.9754\n",
      "  Batch 400: Loss: 0.0662, Accuracy: 0.9745\n",
      "  Batch 500: Loss: 0.0689, Accuracy: 0.9738\n",
      "  Batch 600: Loss: 0.0706, Accuracy: 0.9734\n",
      "  Batch 700: Loss: 0.0696, Accuracy: 0.9738\n",
      "  Batch 800: Loss: 0.0724, Accuracy: 0.9728\n",
      "  Batch 900: Loss: 0.0713, Accuracy: 0.9736\n",
      "  Batch 1000: Loss: 0.0702, Accuracy: 0.9740\n",
      "  Batch 1100: Loss: 0.0718, Accuracy: 0.9736\n",
      "  Batch 1200: Loss: 0.0722, Accuracy: 0.9733\n",
      "  Batch 1300: Loss: 0.0737, Accuracy: 0.9728\n",
      "  Batch 1400: Loss: 0.0743, Accuracy: 0.9728\n",
      "  Batch 1500: Loss: 0.0750, Accuracy: 0.9722\n",
      "  Batch 1600: Loss: 0.0751, Accuracy: 0.9720\n",
      "  Batch 1700: Loss: 0.0745, Accuracy: 0.9724\n",
      "  Batch 1800: Loss: 0.0745, Accuracy: 0.9726\n",
      "  Batch 1900: Loss: 0.0743, Accuracy: 0.9730\n",
      "  Batch 2000: Loss: 0.0755, Accuracy: 0.9726\n",
      "  Batch 2100: Loss: 0.0761, Accuracy: 0.9724\n",
      "  Batch 2200: Loss: 0.0753, Accuracy: 0.9728\n",
      "  Batch 2300: Loss: 0.0759, Accuracy: 0.9726\n",
      "  Batch 2400: Loss: 0.0755, Accuracy: 0.9727\n",
      "  Batch 2500: Loss: 0.0754, Accuracy: 0.9727\n",
      "  Batch 2600: Loss: 0.0752, Accuracy: 0.9728\n",
      "  Batch 2700: Loss: 0.0750, Accuracy: 0.9728\n",
      "  Batch 2800: Loss: 0.0743, Accuracy: 0.9731\n",
      "  Batch 2900: Loss: 0.0741, Accuracy: 0.9731\n",
      "  Batch 3000: Loss: 0.0741, Accuracy: 0.9731\n",
      "  Batch 3100: Loss: 0.0739, Accuracy: 0.9733\n",
      "  Batch 3200: Loss: 0.0734, Accuracy: 0.9736\n",
      "  Batch 3300: Loss: 0.0735, Accuracy: 0.9736\n",
      "  Batch 3400: Loss: 0.0735, Accuracy: 0.9736\n",
      "  Batch 3500: Loss: 0.0740, Accuracy: 0.9735\n",
      "  Batch 3600: Loss: 0.0743, Accuracy: 0.9735\n",
      "  Batch 3700: Loss: 0.0745, Accuracy: 0.9734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b609747c577412d800841e009fb3c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.0743, Accuracy: 0.9734, Time: 5283.27s\n",
      "  Eval Loss: 0.2127, Accuracy: 0.9427, Time: 25.61s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b922d948bd1545daa4ea498cd22c8223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.0483, Accuracy: 0.9812\n",
      "  Batch 200: Loss: 0.0492, Accuracy: 0.9816\n",
      "  Batch 300: Loss: 0.0502, Accuracy: 0.9812\n",
      "  Batch 400: Loss: 0.0530, Accuracy: 0.9794\n",
      "  Batch 500: Loss: 0.0577, Accuracy: 0.9782\n",
      "  Batch 600: Loss: 0.0572, Accuracy: 0.9785\n",
      "  Batch 700: Loss: 0.0574, Accuracy: 0.9786\n",
      "  Batch 800: Loss: 0.0578, Accuracy: 0.9786\n",
      "  Batch 900: Loss: 0.0589, Accuracy: 0.9782\n",
      "  Batch 1000: Loss: 0.0575, Accuracy: 0.9790\n",
      "  Batch 1100: Loss: 0.0588, Accuracy: 0.9788\n",
      "  Batch 1200: Loss: 0.0590, Accuracy: 0.9789\n",
      "  Batch 1300: Loss: 0.0588, Accuracy: 0.9791\n",
      "  Batch 1400: Loss: 0.0585, Accuracy: 0.9791\n",
      "  Batch 1500: Loss: 0.0589, Accuracy: 0.9790\n",
      "  Batch 1600: Loss: 0.0586, Accuracy: 0.9791\n",
      "  Batch 1700: Loss: 0.0583, Accuracy: 0.9789\n",
      "  Batch 1800: Loss: 0.0585, Accuracy: 0.9790\n",
      "  Batch 1900: Loss: 0.0582, Accuracy: 0.9791\n",
      "  Batch 2000: Loss: 0.0577, Accuracy: 0.9793\n",
      "  Batch 2100: Loss: 0.0570, Accuracy: 0.9795\n",
      "  Batch 2200: Loss: 0.0561, Accuracy: 0.9798\n",
      "  Batch 2300: Loss: 0.0560, Accuracy: 0.9798\n",
      "  Batch 2400: Loss: 0.0557, Accuracy: 0.9799\n",
      "  Batch 2500: Loss: 0.0557, Accuracy: 0.9798\n",
      "  Batch 2600: Loss: 0.0550, Accuracy: 0.9801\n",
      "  Batch 2700: Loss: 0.0551, Accuracy: 0.9800\n",
      "  Batch 2800: Loss: 0.0553, Accuracy: 0.9801\n",
      "  Batch 2900: Loss: 0.0558, Accuracy: 0.9801\n",
      "  Batch 3000: Loss: 0.0561, Accuracy: 0.9800\n",
      "  Batch 3100: Loss: 0.0559, Accuracy: 0.9802\n",
      "  Batch 3200: Loss: 0.0561, Accuracy: 0.9802\n",
      "  Batch 3300: Loss: 0.0564, Accuracy: 0.9801\n",
      "  Batch 3400: Loss: 0.0567, Accuracy: 0.9799\n",
      "  Batch 3500: Loss: 0.0564, Accuracy: 0.9801\n",
      "  Batch 3600: Loss: 0.0561, Accuracy: 0.9802\n",
      "  Batch 3700: Loss: 0.0561, Accuracy: 0.9803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b48c89db59447f796dbba23b4a849ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.0564, Accuracy: 0.9801, Time: 5285.39s\n",
      "  Eval Loss: 0.2318, Accuracy: 0.9427, Time: 25.61s\n",
      "\n",
      "Running LoRA experiment...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'is_lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 9\u001b[0m lora_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'is_lora'"
     ]
    }
   ],
   "source": [
    "print(\"Running full fine-tuning experiment...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "full_results = train_model(full_model, train_dataloader, eval_dataloader, full_learning_rate, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c13a9d6-1a59-4ee7-8fdd-e595e5fc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running LoRA experiment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2451704ab23f4b43a5e2036dc23a284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.6871, Accuracy: 0.5513\n",
      "  Batch 200: Loss: 0.6515, Accuracy: 0.5922\n",
      "  Batch 300: Loss: 0.5872, Accuracy: 0.6558\n",
      "  Batch 400: Loss: 0.5237, Accuracy: 0.7077\n",
      "  Batch 500: Loss: 0.4787, Accuracy: 0.7419\n",
      "  Batch 600: Loss: 0.4455, Accuracy: 0.7666\n",
      "  Batch 700: Loss: 0.4231, Accuracy: 0.7826\n",
      "  Batch 800: Loss: 0.4064, Accuracy: 0.7951\n",
      "  Batch 900: Loss: 0.3900, Accuracy: 0.8064\n",
      "  Batch 1000: Loss: 0.3748, Accuracy: 0.8165\n",
      "  Batch 1100: Loss: 0.3649, Accuracy: 0.8244\n",
      "  Batch 1200: Loss: 0.3563, Accuracy: 0.8308\n",
      "  Batch 1300: Loss: 0.3490, Accuracy: 0.8359\n",
      "  Batch 1400: Loss: 0.3415, Accuracy: 0.8404\n",
      "  Batch 1500: Loss: 0.3358, Accuracy: 0.8442\n",
      "  Batch 1600: Loss: 0.3314, Accuracy: 0.8471\n",
      "  Batch 1700: Loss: 0.3257, Accuracy: 0.8506\n",
      "  Batch 1800: Loss: 0.3206, Accuracy: 0.8538\n",
      "  Batch 1900: Loss: 0.3158, Accuracy: 0.8562\n",
      "  Batch 2000: Loss: 0.3115, Accuracy: 0.8589\n",
      "  Batch 2100: Loss: 0.3087, Accuracy: 0.8610\n",
      "  Batch 2200: Loss: 0.3046, Accuracy: 0.8633\n",
      "  Batch 2300: Loss: 0.3018, Accuracy: 0.8650\n",
      "  Batch 2400: Loss: 0.2987, Accuracy: 0.8668\n",
      "  Batch 2500: Loss: 0.2962, Accuracy: 0.8683\n",
      "  Batch 2600: Loss: 0.2935, Accuracy: 0.8698\n",
      "  Batch 2700: Loss: 0.2919, Accuracy: 0.8711\n",
      "  Batch 2800: Loss: 0.2897, Accuracy: 0.8727\n",
      "  Batch 2900: Loss: 0.2876, Accuracy: 0.8741\n",
      "  Batch 3000: Loss: 0.2853, Accuracy: 0.8752\n",
      "  Batch 3100: Loss: 0.2827, Accuracy: 0.8765\n",
      "  Batch 3200: Loss: 0.2812, Accuracy: 0.8774\n",
      "  Batch 3300: Loss: 0.2794, Accuracy: 0.8784\n",
      "  Batch 3400: Loss: 0.2777, Accuracy: 0.8795\n",
      "  Batch 3500: Loss: 0.2755, Accuracy: 0.8804\n",
      "  Batch 3600: Loss: 0.2734, Accuracy: 0.8816\n",
      "  Batch 3700: Loss: 0.2722, Accuracy: 0.8824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b56e887de8e4111bdbc2eb3ef60b919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.2709, Accuracy: 0.8832, Time: 4023.52s\n",
      "  Eval Loss: 0.2108, Accuracy: 0.9278, Time: 26.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0f852741b7484e8b48d8f913ac251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1758, Accuracy: 0.9263\n",
      "  Batch 200: Loss: 0.1838, Accuracy: 0.9234\n",
      "  Batch 300: Loss: 0.1911, Accuracy: 0.9187\n",
      "  Batch 400: Loss: 0.1913, Accuracy: 0.9206\n",
      "  Batch 500: Loss: 0.1957, Accuracy: 0.9186\n",
      "  Batch 600: Loss: 0.1962, Accuracy: 0.9183\n",
      "  Batch 700: Loss: 0.1988, Accuracy: 0.9176\n",
      "  Batch 800: Loss: 0.1986, Accuracy: 0.9177\n",
      "  Batch 900: Loss: 0.1985, Accuracy: 0.9192\n",
      "  Batch 1000: Loss: 0.1994, Accuracy: 0.9194\n",
      "  Batch 1100: Loss: 0.1975, Accuracy: 0.9203\n",
      "  Batch 1200: Loss: 0.1974, Accuracy: 0.9199\n",
      "  Batch 1300: Loss: 0.1972, Accuracy: 0.9205\n",
      "  Batch 1400: Loss: 0.1981, Accuracy: 0.9207\n",
      "  Batch 1500: Loss: 0.1990, Accuracy: 0.9206\n",
      "  Batch 1600: Loss: 0.1984, Accuracy: 0.9210\n",
      "  Batch 1700: Loss: 0.1990, Accuracy: 0.9207\n",
      "  Batch 1800: Loss: 0.1988, Accuracy: 0.9209\n",
      "  Batch 1900: Loss: 0.1987, Accuracy: 0.9211\n",
      "  Batch 2000: Loss: 0.1981, Accuracy: 0.9216\n",
      "  Batch 2100: Loss: 0.1969, Accuracy: 0.9221\n",
      "  Batch 2200: Loss: 0.1973, Accuracy: 0.9220\n",
      "  Batch 2300: Loss: 0.1972, Accuracy: 0.9219\n",
      "  Batch 2400: Loss: 0.1977, Accuracy: 0.9217\n",
      "  Batch 2500: Loss: 0.1978, Accuracy: 0.9214\n",
      "  Batch 2600: Loss: 0.1965, Accuracy: 0.9221\n",
      "  Batch 2700: Loss: 0.1967, Accuracy: 0.9221\n",
      "  Batch 2800: Loss: 0.1972, Accuracy: 0.9220\n",
      "  Batch 2900: Loss: 0.1970, Accuracy: 0.9222\n",
      "  Batch 3000: Loss: 0.1973, Accuracy: 0.9221\n",
      "  Batch 3100: Loss: 0.1974, Accuracy: 0.9221\n",
      "  Batch 3200: Loss: 0.1967, Accuracy: 0.9225\n",
      "  Batch 3300: Loss: 0.1970, Accuracy: 0.9225\n",
      "  Batch 3400: Loss: 0.1971, Accuracy: 0.9224\n",
      "  Batch 3500: Loss: 0.1970, Accuracy: 0.9223\n",
      "  Batch 3600: Loss: 0.1969, Accuracy: 0.9223\n",
      "  Batch 3700: Loss: 0.1964, Accuracy: 0.9224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800aed212f714f77b752d33acfdadf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.1959, Accuracy: 0.9226, Time: 4037.13s\n",
      "  Eval Loss: 0.1972, Accuracy: 0.9358, Time: 26.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046d0b8b38604734b745ae32020fdb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1769, Accuracy: 0.9263\n",
      "  Batch 200: Loss: 0.1726, Accuracy: 0.9322\n",
      "  Batch 300: Loss: 0.1648, Accuracy: 0.9360\n",
      "  Batch 400: Loss: 0.1695, Accuracy: 0.9353\n",
      "  Batch 500: Loss: 0.1683, Accuracy: 0.9356\n",
      "  Batch 600: Loss: 0.1682, Accuracy: 0.9354\n",
      "  Batch 700: Loss: 0.1688, Accuracy: 0.9356\n",
      "  Batch 800: Loss: 0.1685, Accuracy: 0.9343\n",
      "  Batch 900: Loss: 0.1689, Accuracy: 0.9347\n",
      "  Batch 1000: Loss: 0.1684, Accuracy: 0.9345\n",
      "  Batch 1100: Loss: 0.1684, Accuracy: 0.9343\n",
      "  Batch 1200: Loss: 0.1687, Accuracy: 0.9342\n",
      "  Batch 1300: Loss: 0.1681, Accuracy: 0.9346\n",
      "  Batch 1400: Loss: 0.1678, Accuracy: 0.9346\n",
      "  Batch 1500: Loss: 0.1681, Accuracy: 0.9349\n",
      "  Batch 1600: Loss: 0.1699, Accuracy: 0.9342\n",
      "  Batch 1700: Loss: 0.1710, Accuracy: 0.9343\n",
      "  Batch 1800: Loss: 0.1687, Accuracy: 0.9352\n",
      "  Batch 1900: Loss: 0.1701, Accuracy: 0.9346\n",
      "  Batch 2000: Loss: 0.1697, Accuracy: 0.9346\n",
      "  Batch 2100: Loss: 0.1705, Accuracy: 0.9342\n",
      "  Batch 2200: Loss: 0.1705, Accuracy: 0.9342\n",
      "  Batch 2300: Loss: 0.1688, Accuracy: 0.9348\n",
      "  Batch 2400: Loss: 0.1695, Accuracy: 0.9347\n",
      "  Batch 2500: Loss: 0.1696, Accuracy: 0.9348\n",
      "  Batch 2600: Loss: 0.1694, Accuracy: 0.9347\n",
      "  Batch 2700: Loss: 0.1693, Accuracy: 0.9348\n",
      "  Batch 2800: Loss: 0.1688, Accuracy: 0.9350\n",
      "  Batch 2900: Loss: 0.1690, Accuracy: 0.9350\n",
      "  Batch 3000: Loss: 0.1689, Accuracy: 0.9351\n",
      "  Batch 3100: Loss: 0.1691, Accuracy: 0.9353\n",
      "  Batch 3200: Loss: 0.1697, Accuracy: 0.9349\n",
      "  Batch 3300: Loss: 0.1689, Accuracy: 0.9352\n",
      "  Batch 3400: Loss: 0.1683, Accuracy: 0.9355\n",
      "  Batch 3500: Loss: 0.1688, Accuracy: 0.9353\n",
      "  Batch 3600: Loss: 0.1694, Accuracy: 0.9351\n",
      "  Batch 3700: Loss: 0.1695, Accuracy: 0.9351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bda81bb342a4bb6be032f3b0eb83542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.1694, Accuracy: 0.9352, Time: 4033.99s\n",
      "  Eval Loss: 0.2074, Accuracy: 0.9300, Time: 26.23s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9efade5cf9c46648a19a324bedfd061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1506, Accuracy: 0.9425\n",
      "  Batch 200: Loss: 0.1473, Accuracy: 0.9450\n",
      "  Batch 300: Loss: 0.1502, Accuracy: 0.9456\n",
      "  Batch 400: Loss: 0.1500, Accuracy: 0.9447\n",
      "  Batch 500: Loss: 0.1488, Accuracy: 0.9447\n",
      "  Batch 600: Loss: 0.1467, Accuracy: 0.9454\n",
      "  Batch 700: Loss: 0.1467, Accuracy: 0.9452\n",
      "  Batch 800: Loss: 0.1466, Accuracy: 0.9456\n",
      "  Batch 900: Loss: 0.1453, Accuracy: 0.9455\n",
      "  Batch 1000: Loss: 0.1443, Accuracy: 0.9453\n",
      "  Batch 1100: Loss: 0.1457, Accuracy: 0.9448\n",
      "  Batch 1200: Loss: 0.1464, Accuracy: 0.9445\n",
      "  Batch 1300: Loss: 0.1477, Accuracy: 0.9437\n",
      "  Batch 1400: Loss: 0.1478, Accuracy: 0.9441\n",
      "  Batch 1500: Loss: 0.1477, Accuracy: 0.9440\n",
      "  Batch 1600: Loss: 0.1478, Accuracy: 0.9439\n",
      "  Batch 1700: Loss: 0.1476, Accuracy: 0.9439\n",
      "  Batch 1800: Loss: 0.1475, Accuracy: 0.9438\n",
      "  Batch 1900: Loss: 0.1482, Accuracy: 0.9434\n",
      "  Batch 2000: Loss: 0.1490, Accuracy: 0.9430\n",
      "  Batch 2100: Loss: 0.1490, Accuracy: 0.9430\n",
      "  Batch 2200: Loss: 0.1494, Accuracy: 0.9430\n",
      "  Batch 2300: Loss: 0.1496, Accuracy: 0.9430\n",
      "  Batch 2400: Loss: 0.1495, Accuracy: 0.9428\n",
      "  Batch 2500: Loss: 0.1496, Accuracy: 0.9425\n",
      "  Batch 2600: Loss: 0.1502, Accuracy: 0.9423\n",
      "  Batch 2700: Loss: 0.1499, Accuracy: 0.9426\n",
      "  Batch 2800: Loss: 0.1499, Accuracy: 0.9425\n",
      "  Batch 2900: Loss: 0.1506, Accuracy: 0.9424\n",
      "  Batch 3000: Loss: 0.1504, Accuracy: 0.9424\n",
      "  Batch 3100: Loss: 0.1504, Accuracy: 0.9423\n",
      "  Batch 3200: Loss: 0.1508, Accuracy: 0.9422\n",
      "  Batch 3300: Loss: 0.1510, Accuracy: 0.9423\n",
      "  Batch 3400: Loss: 0.1510, Accuracy: 0.9423\n",
      "  Batch 3500: Loss: 0.1514, Accuracy: 0.9423\n",
      "  Batch 3600: Loss: 0.1511, Accuracy: 0.9423\n",
      "  Batch 3700: Loss: 0.1510, Accuracy: 0.9425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caae5d26d434841aeb7745fdb9eda32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.1511, Accuracy: 0.9426, Time: 4113.23s\n",
      "  Eval Loss: 0.2243, Accuracy: 0.9381, Time: 26.88s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700a14975a56435cabb827141215fec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1248, Accuracy: 0.9506\n",
      "  Batch 200: Loss: 0.1290, Accuracy: 0.9525\n",
      "  Batch 300: Loss: 0.1304, Accuracy: 0.9494\n",
      "  Batch 400: Loss: 0.1356, Accuracy: 0.9473\n",
      "  Batch 500: Loss: 0.1379, Accuracy: 0.9461\n",
      "  Batch 600: Loss: 0.1369, Accuracy: 0.9472\n",
      "  Batch 700: Loss: 0.1338, Accuracy: 0.9486\n",
      "  Batch 800: Loss: 0.1330, Accuracy: 0.9488\n",
      "  Batch 900: Loss: 0.1333, Accuracy: 0.9494\n",
      "  Batch 1000: Loss: 0.1332, Accuracy: 0.9503\n",
      "  Batch 1100: Loss: 0.1331, Accuracy: 0.9502\n",
      "  Batch 1200: Loss: 0.1326, Accuracy: 0.9499\n",
      "  Batch 1300: Loss: 0.1337, Accuracy: 0.9496\n",
      "  Batch 1400: Loss: 0.1353, Accuracy: 0.9489\n",
      "  Batch 1500: Loss: 0.1358, Accuracy: 0.9485\n",
      "  Batch 1600: Loss: 0.1350, Accuracy: 0.9490\n",
      "  Batch 1700: Loss: 0.1353, Accuracy: 0.9489\n",
      "  Batch 1800: Loss: 0.1343, Accuracy: 0.9489\n",
      "  Batch 1900: Loss: 0.1352, Accuracy: 0.9487\n",
      "  Batch 2000: Loss: 0.1350, Accuracy: 0.9487\n",
      "  Batch 2100: Loss: 0.1357, Accuracy: 0.9483\n",
      "  Batch 2200: Loss: 0.1350, Accuracy: 0.9486\n",
      "  Batch 2300: Loss: 0.1354, Accuracy: 0.9483\n",
      "  Batch 2400: Loss: 0.1352, Accuracy: 0.9483\n",
      "  Batch 2500: Loss: 0.1350, Accuracy: 0.9486\n",
      "  Batch 2600: Loss: 0.1353, Accuracy: 0.9483\n",
      "  Batch 2700: Loss: 0.1360, Accuracy: 0.9481\n",
      "  Batch 2800: Loss: 0.1361, Accuracy: 0.9482\n",
      "  Batch 2900: Loss: 0.1367, Accuracy: 0.9480\n",
      "  Batch 3000: Loss: 0.1371, Accuracy: 0.9479\n",
      "  Batch 3100: Loss: 0.1369, Accuracy: 0.9480\n",
      "  Batch 3200: Loss: 0.1372, Accuracy: 0.9478\n",
      "  Batch 3300: Loss: 0.1370, Accuracy: 0.9479\n",
      "  Batch 3400: Loss: 0.1368, Accuracy: 0.9479\n",
      "  Batch 3500: Loss: 0.1373, Accuracy: 0.9478\n",
      "  Batch 3600: Loss: 0.1370, Accuracy: 0.9480\n",
      "  Batch 3700: Loss: 0.1372, Accuracy: 0.9480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44934844d1d5487fa7fc066868d5f8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.1376, Accuracy: 0.9480, Time: 4154.43s\n",
      "  Eval Loss: 0.2179, Accuracy: 0.9369, Time: 26.81s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning LoRA experiment...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lora_results = train_model(lora_model, train_dataloader, eval_dataloader, lora_learning_rate, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b055f070-6fda-4ea3-abc2-22090cc63fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Full:\n",
      "  Total training time: 26491.80s\n",
      "  Total evaluation time: 128.54s\n",
      "  Peak GPU memory usage: 2.02GB\n",
      "  Final evaluation accuracy: 0.9427\n",
      "LoRA:\n",
      "  Total training time: 20362.31s\n",
      "  Total evaluation time: 132.35s\n",
      "  Peak GPU memory usage: 1.02GB\n",
      "  Final evaluation accuracy: 0.9369\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerformance Comparison:\")\n",
    "print_train_results(\"Full\", full_results)\n",
    "print_train_results(\"LoRA\", lora_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4daa1-03ac-4be9-8a9f-73c2c7b6f5e6",
   "metadata": {},
   "source": [
    "### Evaluate and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abc3283-e362-4d11-abbc-65a5ebd2e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating full fine-tuned model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1bd04cb1af41ccb606384737957772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model - Accuracy: 0.9538, Loss: 0.1755, Eval time: 197.12s\n",
      "\n",
      "Evaluating LoRA model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242510c9055d4286a965f459fb472ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model - Accuracy: 0.9468, Loss: 0.1553, Eval time: 205.08s\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating full fine-tuned model:\")\n",
    "full_loss, full_accuracy, full_eval_time = evaluate(full_model, test_dataloader, device)\n",
    "print(f\"Full model - Accuracy: {full_accuracy:.4f}, Loss: {full_loss:.4f}, Eval time: {full_eval_time:.2f}s\")\n",
    "\n",
    "print(\"\\nEvaluating LoRA model:\")\n",
    "lora_loss, lora_accuracy, lora_eval_time = evaluate(lora_model, test_dataloader, device)\n",
    "print(f\"LoRA model - Accuracy: {lora_accuracy:.4f}, Loss: {lora_loss:.4f}, Eval time: {lora_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "067052f0-5d6b-4761-a893-80af46175293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full fine-tuned model saved to ./sst2-full_finetuned_model.pt\n",
      "Full model size: 475.57 MB\n",
      "LoRA layers and classifier saved to ./sst2-lora_and_classifier.pt\n",
      "LoRA model size: 3.40 MB\n",
      "\n",
      "Size reduction: 139.75x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "full_save_path = \"./sst2-full_finetuned_model.pt\"\n",
    "torch.save(full_model.state_dict(), full_save_path)\n",
    "full_model.save_pretrained(\"sst2-full_finetuned_model\")\n",
    "full_model_size = os.path.getsize(full_save_path) / (1024 * 1024)  # Size in MB\n",
    "print(f\"\\nFull fine-tuned model saved to {full_save_path}\")\n",
    "print(f\"Full model size: {full_model_size:.2f} MB\")\n",
    "\n",
    "lora_save_path = \"./sst2-lora_and_classifier.pt\"\n",
    "lora_state_dict = {name: param for name, param in lora_model.named_parameters() \n",
    "                   if 'lora_A' in name or 'lora_B' in name or 'classifier' in name}\n",
    "torch.save(lora_state_dict, lora_save_path)\n",
    "lora_model_size = os.path.getsize(lora_save_path) / (1024 * 1024)  # Size in MB\n",
    "print(f\"LoRA layers and classifier saved to {lora_save_path}\")\n",
    "print(f\"LoRA model size: {lora_model_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nSize reduction: {full_model_size / lora_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded824b5-f326-40d2-b8c2-1537a5c0ab33",
   "metadata": {},
   "source": [
    "### Load Model from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a90cdaea-dcf3-491f-8ac2-18d6ca7a4691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1953e70afdfb4e99b53d5f4b30f6b191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full model - Accuracy: 0.9538, Loss: 0.1755, Eval time: 191.10s\n"
     ]
    }
   ],
   "source": [
    "# Load full fine-tuned model\n",
    "loaded_full_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "loaded_full_model.load_state_dict(torch.load(full_save_path))\n",
    "loaded_full_model.to(device)\n",
    "\n",
    "# Evaluate loaded full model\n",
    "full_loss, full_accuracy, full_eval_time = evaluate(loaded_full_model, test_dataloader, device)\n",
    "print(f\"Loaded full model - Accuracy: {full_accuracy:.4f}, Loss: {full_loss:.4f}, Eval time: {full_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14c824ee-2b76-4f3e-a426-7f333906bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b51b29d17a43b4be04c70394c14e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LoRA model - Accuracy: 0.9468, Loss: 0.1553, Eval time: 201.92s\n"
     ]
    }
   ],
   "source": [
    "# Load LoRA model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "loaded_lora_model = apply_lora(base_model, rank=8, alpha=8)\n",
    "loaded_lora_model.load_state_dict(torch.load(lora_save_path), strict=False)\n",
    "loaded_lora_model.to(device)\n",
    "\n",
    "# Evaluate loaded LoRA model\n",
    "lora_loss, lora_accuracy, lora_eval_time = evaluate(loaded_lora_model, test_dataloader, device)\n",
    "print(f\"Loaded LoRA model - Accuracy: {lora_accuracy:.4f}, Loss: {lora_loss:.4f}, Eval time: {lora_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6ad093-9abe-4503-8a4c-f199ab7521f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying LoRA layers:\n",
      "LoRA layer found: roberta.encoder.layer.0.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.0.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.1.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.1.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.2.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.2.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.3.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.3.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.4.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.4.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.5.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.5.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.6.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.6.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.7.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.7.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.8.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.8.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.9.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.9.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.10.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.10.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.11.attention.self.query\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "LoRA layer found: roberta.encoder.layer.11.attention.self.value\n",
      "lora_A shape: torch.Size([768, 8])\n",
      "lora_B shape: torch.Size([8, 768])\n",
      "\n",
      "Evaluating model with zeroed LoRA weights:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0fd6a0ad074882bd4a4eb5cd37c6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroed LoRA model - Accuracy: 0.5543, Loss: 0.6874, Eval time: 200.53s\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate that LoRA is there and that without it the model performs poorly\n",
    "\n",
    "print(\"\\nVerifying LoRA layers:\")\n",
    "for name, module in loaded_lora_model.named_modules():\n",
    "    if isinstance(module, LoRALayer):\n",
    "        print(f\"LoRA layer found: {name}\")\n",
    "        print(f\"lora_A shape: {module.lora_A.shape}\")\n",
    "        print(f\"lora_B shape: {module.lora_B.shape}\")\n",
    "        print()\n",
    "\n",
    "print(\"Evaluating model with zeroed LoRA weights:\")\n",
    "for name, param in loaded_lora_model.named_parameters():\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        param.data.zero_()\n",
    "\n",
    "zero_loss, zero_accuracy, zero_eval_time = evaluate(loaded_lora_model, test_dataloader, device)\n",
    "print(f\"Zeroed LoRA model - Accuracy: {zero_accuracy:.4f}, Loss: {zero_loss:.4f}, Eval time: {zero_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed47dcfd-0444-4382-a35b-2b6d1ff3f85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
