{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7cf164-2fe2-4061-81b4-fbdfc2f34f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "from IPython.display import clear_output\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d70c48-1c67-4213-a98d-3e7c7ac26e0b",
   "metadata": {},
   "source": [
    "# LoRA Setup\n",
    "\n",
    "First, we need to setup a LoRALayer class which we can then inject into our transformer model, switching out only the query and value attention layers as stated in the original paper. The following code should work on any transformer model and therefore is model-agnostic, as long as the query and value layers hold \"query\" and \"value\" in their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ff64fc-a4f3-4c02-939c-d9e54cb695b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4, alpha=1):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.randn(original_layer.in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, original_layer.out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_output = self.original_layer(x)\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        return original_output + lora_output\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(x in name for x in ['query', 'value']):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            child_name = name.split('.')[-1]\n",
    "            parent = model.get_submodule(parent_name)\n",
    "            lora_layer = LoRALayer(module, rank, alpha)\n",
    "            setattr(parent, child_name, lora_layer)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_A' in name or 'lora_B' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b92aec5-e55d-4561-869a-4863c67a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (predictions == batch['labels']).sum().item()\n",
    "        total_samples += batch['labels'].size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_loss = total_loss / (batch_idx + 1)\n",
    "            current_accuracy = total_correct / total_samples\n",
    "            print(f\"  Batch {batch_idx+1}: Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.4f}\")\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy, epoch_time\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (all_predictions == all_labels).mean()\n",
    "    \n",
    "    # Calculate Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    \n",
    "    return avg_loss, accuracy, mcc, f1, eval_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed85a32-a347-4c16-af9a-c85432dd2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, eval_dataloader, learning_rate, num_epochs, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=250, num_training_steps=len(train_dataloader) * num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_losses = []\n",
    "    eval_accuracies = []\n",
    "    \n",
    "    total_train_time = 0\n",
    "    total_eval_time = 0\n",
    "    peak_memory_usage = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_accuracy, train_time = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        total_train_time += train_time\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Update peak memory usage\n",
    "        current_memory = torch.cuda.memory_allocated() / 1e9  # Convert to GB\n",
    "        peak_memory_usage = max(peak_memory_usage, current_memory)\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_loss, eval_accuracy, eval_time = evaluate(model, eval_dataloader, device)\n",
    "        total_eval_time += eval_time\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Time: {train_time:.2f}s\")\n",
    "        print(f\"  Eval Loss: {eval_loss:.4f}, Accuracy: {eval_accuracy:.4f}, Time: {eval_time:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'eval_losses': eval_losses,\n",
    "        'eval_accuracies': eval_accuracies,\n",
    "        'total_train_time': total_train_time,\n",
    "        'total_eval_time': total_eval_time,\n",
    "        'peak_memory_usage': peak_memory_usage,\n",
    "        'final_train_accuracy': train_accuracies[-1],\n",
    "        'final_eval_accuracy': eval_accuracies[-1]\n",
    "    }\n",
    "\n",
    "def print_train_results(name, results):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total training time: {results['total_train_time']:.2f}s\")\n",
    "    print(f\"  Total evaluation time: {results['total_eval_time']:.2f}s\")\n",
    "    print(f\"  Peak GPU memory usage: {results['peak_memory_usage']:.2f}GB\")\n",
    "    print(f\"  Final evaluation accuracy: {results['eval_accuracies'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71248bfc-8fc3-4482-b16c-6076eaf4b792",
   "metadata": {},
   "source": [
    "# Reproducing CoLA with RoBeRTa-base\n",
    "\n",
    "We restrict ourselves to comparing full fine-tuning with LoRA on RoBeRTa-base, testing CoLA (this notebook) and SST-2 MRPC (other notebooks) of the GLUE benchmark. Because we do not have the resources to create a proper submission for GLUE and therefore also cannot do a proper evaluation on the test split, we create our own test split from part of the train split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363344a-92a0-4939-a25f-d41f69954971",
   "metadata": {},
   "source": [
    "## SST-2\n",
    "\n",
    "In this notebook, we reproduce the Corpus of Linguistic Acceptability (CoLA), which contains sentences and a label whether they are linguistically acceptable or not, therefore being a binary classification task. As in the paper, we will use  a learning rate of 4e-4 for LoRA, a rank of 8, alpha of 8 and a maximum sequence length of 512. Different to the paper (because of our memory limitations), we use a batch size of 16 instead of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1becc97-2a51-4472-81e4-bcf9851be6cf",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7c88ec-f792-4671-b1ee-4a9250f09e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "full_learning_rate = 1e-5\n",
    "lora_learning_rate = 4e-4\n",
    "rank = 8\n",
    "alpha = 8\n",
    "max_sequence_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc7755a-73c5-4bf7-8edd-ab15885b1a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d02d9a32a340aaa89aaa383efd0653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18afd8e8c03547b89986da3e3c72e023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"glue\", \"cola\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_sequence_length)\n",
    "\n",
    "split = raw_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "dataset = split\n",
    "dataset[\"validation\"] = raw_dataset[\"validation\"]\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e26826-112f-4860-b4d4-ec36b6c2dcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine-tuning trainable parameters: 124647170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA trainable parameters: 294912\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Full fine-tuning setup\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\n",
    "full_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)\n",
    "print(f\"Full fine-tuning trainable parameters: {full_params}\")\n",
    "\n",
    "# LoRA setup\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "lora_model = apply_lora(lora_model, rank=rank, alpha=alpha).to(device)\n",
    "lora_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA trainable parameters: {lora_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a01d00-fe1c-4e38-abb3-9bf8549b19b8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4533ce8c-b45d-4b22-a42c-7b30d9fda7f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full fine-tuning experiment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db98283c9ac64d68b403da0fd0e9389e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.7176, Accuracy: 0.4363\n",
      "  Batch 200: Loss: 0.6476, Accuracy: 0.5722\n",
      "  Batch 300: Loss: 0.5981, Accuracy: 0.6367\n",
      "  Batch 400: Loss: 0.5671, Accuracy: 0.6725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da8a550063840fe8828781644f8f608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.5468, Accuracy: 0.6923, Time: 684.18s\n",
      "  Eval Loss: 0.4822, Accuracy: 0.7958, Time: 31.92s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45bd6da85b34b6586b8bbdd73a57a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.3447, Accuracy: 0.8544\n",
      "  Batch 200: Loss: 0.3526, Accuracy: 0.8519\n",
      "  Batch 300: Loss: 0.3567, Accuracy: 0.8502\n",
      "  Batch 400: Loss: 0.3529, Accuracy: 0.8528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d767738302e24c9e85b514e8bbba1718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.3489, Accuracy: 0.8535, Time: 688.15s\n",
      "  Eval Loss: 0.3890, Accuracy: 0.8341, Time: 31.90s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f41bc537b94c9f99dc7d14df110f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.2351, Accuracy: 0.9113\n",
      "  Batch 200: Loss: 0.2489, Accuracy: 0.9059\n",
      "  Batch 300: Loss: 0.2428, Accuracy: 0.9079\n",
      "  Batch 400: Loss: 0.2388, Accuracy: 0.9094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5de2811bd640548a10aff0f7153476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.2360, Accuracy: 0.9101, Time: 687.97s\n",
      "  Eval Loss: 0.4907, Accuracy: 0.8303, Time: 31.88s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d607c42d4a496ea0d369b7eb727b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1470, Accuracy: 0.9425\n",
      "  Batch 200: Loss: 0.1576, Accuracy: 0.9413\n",
      "  Batch 300: Loss: 0.1601, Accuracy: 0.9429\n",
      "  Batch 400: Loss: 0.1617, Accuracy: 0.9408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34799f23e648a3803040bc660e7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.1608, Accuracy: 0.9414, Time: 688.06s\n",
      "  Eval Loss: 0.5087, Accuracy: 0.8370, Time: 31.91s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babcd2acec8d48c4b17bfe7bd21f48b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.1162, Accuracy: 0.9656\n",
      "  Batch 200: Loss: 0.1135, Accuracy: 0.9647\n",
      "  Batch 300: Loss: 0.1158, Accuracy: 0.9608\n",
      "  Batch 400: Loss: 0.1213, Accuracy: 0.9591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec18cc855254fe48a6913ba94715e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.1211, Accuracy: 0.9593, Time: 688.19s\n",
      "  Eval Loss: 0.5679, Accuracy: 0.8389, Time: 31.92s\n"
     ]
    }
   ],
   "source": [
    "print(\"Running full fine-tuning experiment...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "full_results = train_model(full_model, train_dataloader, eval_dataloader, full_learning_rate, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c13a9d6-1a59-4ee7-8fdd-e595e5fc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running LoRA experiment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff446a0ac153435e94fc59d090db8c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.7213, Accuracy: 0.3875\n",
      "  Batch 200: Loss: 0.6588, Accuracy: 0.5469\n",
      "  Batch 300: Loss: 0.6146, Accuracy: 0.6110\n",
      "  Batch 400: Loss: 0.5813, Accuracy: 0.6514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e649647f90c48d89fcd7c16e473bc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Train Loss: 0.5606, Accuracy: 0.6747, Time: 522.26s\n",
      "  Eval Loss: 0.4513, Accuracy: 0.8025, Time: 32.21s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c696ee29cef944e38db9b986f5feb32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.4105, Accuracy: 0.8094\n",
      "  Batch 200: Loss: 0.4223, Accuracy: 0.8075\n",
      "  Batch 300: Loss: 0.4197, Accuracy: 0.8085\n",
      "  Batch 400: Loss: 0.4212, Accuracy: 0.8069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626799d5eda84921bee7310fd8deb3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "  Train Loss: 0.4182, Accuracy: 0.8092, Time: 522.05s\n",
      "  Eval Loss: 0.5260, Accuracy: 0.8063, Time: 32.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb042c4db3564b999915831e538ffb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.3617, Accuracy: 0.8444\n",
      "  Batch 200: Loss: 0.3853, Accuracy: 0.8287\n",
      "  Batch 300: Loss: 0.3757, Accuracy: 0.8344\n",
      "  Batch 400: Loss: 0.3790, Accuracy: 0.8298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489acf2f30d445ffb788b108461392a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "  Train Loss: 0.3799, Accuracy: 0.8296, Time: 522.23s\n",
      "  Eval Loss: 0.4819, Accuracy: 0.8092, Time: 32.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f9ed0c19b5495989a88d7f8f887c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.3446, Accuracy: 0.8494\n",
      "  Batch 200: Loss: 0.3547, Accuracy: 0.8500\n",
      "  Batch 300: Loss: 0.3502, Accuracy: 0.8496\n",
      "  Batch 400: Loss: 0.3537, Accuracy: 0.8494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4396ff740ff44da88a30d7791dd5a6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "  Train Loss: 0.3510, Accuracy: 0.8498, Time: 522.33s\n",
      "  Eval Loss: 0.4745, Accuracy: 0.8236, Time: 32.24s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881ed4280aff4fb1a12ec4818108d964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100: Loss: 0.3550, Accuracy: 0.8544\n",
      "  Batch 200: Loss: 0.3510, Accuracy: 0.8516\n",
      "  Batch 300: Loss: 0.3445, Accuracy: 0.8558\n",
      "  Batch 400: Loss: 0.3440, Accuracy: 0.8556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8495483773d4488f87d948dbe15b58fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "  Train Loss: 0.3429, Accuracy: 0.8555, Time: 522.04s\n",
      "  Eval Loss: 0.4703, Accuracy: 0.8207, Time: 32.18s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning LoRA experiment...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "lora_results = train_model(lora_model, train_dataloader, eval_dataloader, lora_learning_rate, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b055f070-6fda-4ea3-abc2-22090cc63fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Full:\n",
      "  Total training time: 3436.54s\n",
      "  Total evaluation time: 159.54s\n",
      "  Peak GPU memory usage: 2.02GB\n",
      "  Final evaluation accuracy: 0.8389\n",
      "LoRA:\n",
      "  Total training time: 2610.91s\n",
      "  Total evaluation time: 161.06s\n",
      "  Peak GPU memory usage: 1.02GB\n",
      "  Final evaluation accuracy: 0.8207\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerformance Comparison:\")\n",
    "print_train_results(\"Full\", full_results)\n",
    "print_train_results(\"LoRA\", lora_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4daa1-03ac-4be9-8a9f-73c2c7b6f5e6",
   "metadata": {},
   "source": [
    "### Evaluate and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9abc3283-e362-4d11-abbc-65a5ebd2e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating full fine-tuned model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4ccbfe7cae4f90911d5ccc261d2922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model - Accuracy: 0.8481, Loss: 0.5417, MCC: 0.6285, F1: 0.8962, Eval time: 24.92s\n",
      "\n",
      "Evaluating LoRA model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2789ad5024db412b8ca87256681dd681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model - Accuracy: 0.8107, Loss: 0.4678, MCC: 0.5289, F1: 0.8728, Eval time: 25.92s\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating full fine-tuned model:\")\n",
    "full_loss, full_accuracy, full_mcc, full_f1, full_eval_time = evaluate(full_model, test_dataloader, device)\n",
    "print(f\"Full model - Accuracy: {full_accuracy:.4f}, Loss: {full_loss:.4f}, MCC: {full_mcc:.4f}, F1: {full_f1:.4f}, Eval time: {full_eval_time:.2f}s\")\n",
    "\n",
    "print(\"\\nEvaluating LoRA model:\")\n",
    "lora_loss, lora_accuracy, lora_mcc, lora_f1, lora_eval_time = evaluate(lora_model, test_dataloader, device)\n",
    "print(f\"LoRA model - Accuracy: {lora_accuracy:.4f}, Loss: {lora_loss:.4f}, MCC: {lora_mcc:.4f}, F1: {lora_f1:.4f}, Eval time: {lora_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "067052f0-5d6b-4761-a893-80af46175293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full fine-tuned model saved to ./cola-full_finetuned_model.pt\n",
      "Full model size: 475.57 MB\n",
      "LoRA layers and classifier saved to ./cola-lora_and_classifier.pt\n",
      "LoRA model size: 3.40 MB\n",
      "\n",
      "Size reduction: 139.75x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "full_save_path = \"./cola-full_finetuned_model.pt\"\n",
    "torch.save(full_model.state_dict(), full_save_path)\n",
    "full_model_size = os.path.getsize(full_save_path) / (1024 * 1024)  # Size in MB\n",
    "print(f\"\\nFull fine-tuned model saved to {full_save_path}\")\n",
    "print(f\"Full model size: {full_model_size:.2f} MB\")\n",
    "\n",
    "lora_save_path = \"./cola-lora_and_classifier.pt\"\n",
    "lora_state_dict = {name: param for name, param in lora_model.named_parameters() \n",
    "                   if 'lora_A' in name or 'lora_B' in name or 'classifier' in name}\n",
    "torch.save(lora_state_dict, lora_save_path)\n",
    "lora_model_size = os.path.getsize(lora_save_path) / (1024 * 1024)  # Size in MB\n",
    "print(f\"LoRA layers and classifier saved to {lora_save_path}\")\n",
    "print(f\"LoRA model size: {lora_model_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nSize reduction: {full_model_size / lora_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded824b5-f326-40d2-b8c2-1537a5c0ab33",
   "metadata": {},
   "source": [
    "### Load Model from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cdaea-dcf3-491f-8ac2-18d6ca7a4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full fine-tuned model\n",
    "loaded_full_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "loaded_full_model.load_state_dict(torch.load(full_save_path))\n",
    "loaded_full_model.to(device)\n",
    "\n",
    "# Evaluate loaded full model\n",
    "full_loss, full_accuracy, full_eval_time = evaluate(loaded_full_model, test_dataloader, device)\n",
    "print(f\"Loaded full model - Accuracy: {full_accuracy:.4f}, Loss: {full_loss:.4f}, Eval time: {full_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c824ee-2b76-4f3e-a426-7f333906bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "loaded_lora_model = apply_lora(base_model, rank=8, alpha=8)\n",
    "loaded_lora_model.load_state_dict(torch.load(lora_save_path), strict=False)\n",
    "loaded_lora_model.to(device)\n",
    "\n",
    "# Evaluate loaded LoRA model\n",
    "lora_loss, lora_accuracy, lora_eval_time = evaluate(loaded_lora_model, test_dataloader, device)\n",
    "print(f\"Loaded LoRA model - Accuracy: {lora_accuracy:.4f}, Loss: {lora_loss:.4f}, Eval time: {lora_eval_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ad093-9abe-4503-8a4c-f199ab7521f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demonstrate that LoRA is there and that without it the model performs poorly\n",
    "\n",
    "print(\"\\nVerifying LoRA layers:\")\n",
    "for name, module in loaded_lora_model.named_modules():\n",
    "    if isinstance(module, LoRALayer):\n",
    "        print(f\"LoRA layer found: {name}\")\n",
    "        print(f\"lora_A shape: {module.lora_A.shape}\")\n",
    "        print(f\"lora_B shape: {module.lora_B.shape}\")\n",
    "        print()\n",
    "\n",
    "print(\"Evaluating model with zeroed LoRA weights:\")\n",
    "for name, param in loaded_lora_model.named_parameters():\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        param.data.zero_()\n",
    "\n",
    "zero_loss, zero_accuracy, zero_eval_time = evaluate(loaded_lora_model, test_dataloader, device)\n",
    "print(f\"Zeroed LoRA model - Accuracy: {zero_accuracy:.4f}, Loss: {zero_loss:.4f}, Eval time: {zero_eval_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
