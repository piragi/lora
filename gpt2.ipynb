{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, weight, r, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        out_features = self.weight.shape[0]\n",
    "        in_features = self.weight.shape[1]\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, in_features))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(out_features, r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = x @ self.weight.T\n",
    "        result += x @ (self.A.T @ self.B.T)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2370571345090866\n",
      "Epoch 20, Loss: 0.18878374993801117\n",
      "Epoch 30, Loss: 0.14447425305843353\n",
      "Epoch 40, Loss: 0.15330176055431366\n",
      "Epoch 50, Loss: 0.03857145458459854\n",
      "Epoch 60, Loss: 0.028042878955602646\n",
      "Epoch 70, Loss: 0.04426877945661545\n",
      "Epoch 80, Loss: 0.017143048346042633\n",
      "Epoch 90, Loss: 0.02256743796169758\n",
      "Epoch 100, Loss: 0.009024682454764843\n",
      "Epoch 110, Loss: 0.012838989496231079\n",
      "Epoch 120, Loss: 0.005366366356611252\n",
      "Epoch 130, Loss: 0.008101089857518673\n",
      "Epoch 140, Loss: 0.006729419343173504\n",
      "Epoch 150, Loss: 0.0031156884506344795\n",
      "Epoch 160, Loss: 0.004780036862939596\n",
      "Epoch 170, Loss: 0.005099200643599033\n",
      "Epoch 180, Loss: 0.002020863350480795\n",
      "Epoch 190, Loss: 0.0017750142142176628\n",
      "Epoch 200, Loss: 0.0027535418048501015\n",
      "Epoch 210, Loss: 0.0013972821179777384\n",
      "Epoch 220, Loss: 0.0012848370242863894\n",
      "Epoch 230, Loss: 0.002415280556306243\n",
      "Epoch 240, Loss: 0.001051163300871849\n",
      "Epoch 250, Loss: 0.0009390150080434978\n",
      "Epoch 260, Loss: 0.0018143825000151992\n",
      "Epoch 270, Loss: 0.0013639103854075074\n",
      "Epoch 280, Loss: 0.0012408702168613672\n",
      "Epoch 290, Loss: 0.001156582380644977\n",
      "Epoch 300, Loss: 0.0010594234336167574\n",
      "Epoch 310, Loss: 0.0009873358067125082\n",
      "Epoch 320, Loss: 0.0005362694500945508\n",
      "Epoch 330, Loss: 0.000506788317579776\n",
      "Epoch 340, Loss: 0.0004707022453658283\n",
      "Epoch 350, Loss: 0.00044442492071539164\n",
      "Epoch 360, Loss: 0.0008496014052070677\n",
      "Epoch 370, Loss: 0.000393018068280071\n",
      "Epoch 380, Loss: 0.00037010275991633534\n",
      "Epoch 390, Loss: 0.0007064965320751071\n",
      "Epoch 400, Loss: 0.0006603187066502869\n",
      "Input: [[0. 0.]], Predicted: [[0.01800893]]\n",
      "Input: [[1. 1.]], Predicted: [[0.02567577]]\n",
      "Input: [[0. 1.]], Predicted: [[0.9766028]]\n",
      "Input: [[1. 0.]], Predicted: [[0.9818619]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "ffn = FFN(2, 16, 1)\n",
    "x_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_xor)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "def train_xor_model(model, dataloader):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(400):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "def validate_xor_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for predictions\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            print(f\"Input: {inputs.numpy()}, Predicted: {outputs.numpy()}\")\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6913596391677856\n",
      "Epoch 20, Loss: 1.1999181879218668e-05\n",
      "Epoch 30, Loss: 0.21014522016048431\n",
      "Epoch 40, Loss: 3.653912372669765e-08\n",
      "Epoch 50, Loss: 0.16833238303661346\n",
      "Epoch 60, Loss: 1.4353318533721904e-08\n",
      "Epoch 70, Loss: 0.03315728157758713\n",
      "Epoch 80, Loss: 9.094947017729282e-09\n",
      "Epoch 90, Loss: 2.1443704554258147e-06\n",
      "Epoch 100, Loss: 2.111504272761522e-06\n",
      "Epoch 110, Loss: 0.08623956143856049\n",
      "Epoch 120, Loss: 2.0042900814587483e-06\n",
      "Epoch 130, Loss: 5.150070592208067e-09\n",
      "Epoch 140, Loss: 1.8691039258555975e-06\n",
      "Epoch 150, Loss: 0.010944991372525692\n",
      "Epoch 160, Loss: 1.7177943618662539e-06\n",
      "Epoch 170, Loss: 1.6434642020612955e-06\n",
      "Epoch 180, Loss: 0.008251631632447243\n",
      "Epoch 190, Loss: 3.2741809263825417e-09\n",
      "Epoch 200, Loss: 0.006990970578044653\n",
      "Epoch 210, Loss: 1.3510579037756543e-06\n",
      "Epoch 220, Loss: 2.7138327141074114e-09\n",
      "Epoch 230, Loss: 0.026804056018590927\n",
      "Epoch 240, Loss: 0.005128184799104929\n",
      "Epoch 250, Loss: 2.3194388631964102e-09\n",
      "Epoch 260, Loss: 0.021439911797642708\n",
      "Epoch 270, Loss: 0.004190040752291679\n",
      "Epoch 280, Loss: 0.018683351576328278\n",
      "Epoch 290, Loss: 8.978226446743065e-07\n",
      "Epoch 300, Loss: 1.8111592225977802e-09\n",
      "Epoch 310, Loss: 1.7408297026122455e-09\n",
      "Epoch 320, Loss: 0.014506101608276367\n",
      "Epoch 330, Loss: 0.013662750832736492\n",
      "Epoch 340, Loss: 0.01288397703319788\n",
      "Epoch 350, Loss: 0.002609542803838849\n",
      "Epoch 360, Loss: 6.414530275833386e-07\n",
      "Epoch 370, Loss: 6.137836408015573e-07\n",
      "Epoch 380, Loss: 5.849906301591545e-07\n",
      "Epoch 390, Loss: 1.2619807421287987e-09\n",
      "Epoch 400, Loss: 5.384518431128527e-07\n",
      "Input: [[0. 1.]], Predicted: [[0.9992667]]\n",
      "Input: [[1. 0.]], Predicted: [[0.9999651]]\n",
      "Input: [[1. 1.]], Predicted: [[0.954939]]\n",
      "Input: [[0. 0.]], Predicted: [[0.09654544]]\n"
     ]
    }
   ],
   "source": [
    "ffn_weight = ffn.linear1.weight.detach().clone()\n",
    "lora_layer = LoRALayer(ffn_weight, 1, 0.1)\n",
    "setattr(ffn, 'linear1', lora_layer)\n",
    "\n",
    "y_or = torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_or)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of FFN(\n",
       "  (linear1): LoRALayer()\n",
       "  (linear2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piragi/projects/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "loading file merges.txt from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "loading configuration file config.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/piragi/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT2LMHeadModel\n",
      "transformer GPT2Model\n",
      "transformer.wte Embedding\n",
      "transformer.wpe Embedding\n",
      "transformer.drop Dropout\n",
      "transformer.h ModuleList\n",
      "transformer.h.0 GPT2Block\n",
      "transformer.h.0.ln_1 LayerNorm\n",
      "transformer.h.0.attn GPT2Attention\n",
      "transformer.h.0.attn.c_attn Conv1D\n",
      "transformer.h.0.attn.c_proj Conv1D\n",
      "transformer.h.0.attn.attn_dropout Dropout\n",
      "transformer.h.0.attn.resid_dropout Dropout\n",
      "transformer.h.0.ln_2 LayerNorm\n",
      "transformer.h.0.mlp GPT2MLP\n",
      "transformer.h.0.mlp.c_fc Conv1D\n",
      "transformer.h.0.mlp.c_proj Conv1D\n",
      "transformer.h.0.mlp.act NewGELUActivation\n",
      "transformer.h.0.mlp.dropout Dropout\n",
      "transformer.h.1 GPT2Block\n",
      "transformer.h.1.ln_1 LayerNorm\n",
      "transformer.h.1.attn GPT2Attention\n",
      "transformer.h.1.attn.c_attn Conv1D\n",
      "transformer.h.1.attn.c_proj Conv1D\n",
      "transformer.h.1.attn.attn_dropout Dropout\n",
      "transformer.h.1.attn.resid_dropout Dropout\n",
      "transformer.h.1.ln_2 LayerNorm\n",
      "transformer.h.1.mlp GPT2MLP\n",
      "transformer.h.1.mlp.c_fc Conv1D\n",
      "transformer.h.1.mlp.c_proj Conv1D\n",
      "transformer.h.1.mlp.act NewGELUActivation\n",
      "transformer.h.1.mlp.dropout Dropout\n",
      "transformer.h.2 GPT2Block\n",
      "transformer.h.2.ln_1 LayerNorm\n",
      "transformer.h.2.attn GPT2Attention\n",
      "transformer.h.2.attn.c_attn Conv1D\n",
      "transformer.h.2.attn.c_proj Conv1D\n",
      "transformer.h.2.attn.attn_dropout Dropout\n",
      "transformer.h.2.attn.resid_dropout Dropout\n",
      "transformer.h.2.ln_2 LayerNorm\n",
      "transformer.h.2.mlp GPT2MLP\n",
      "transformer.h.2.mlp.c_fc Conv1D\n",
      "transformer.h.2.mlp.c_proj Conv1D\n",
      "transformer.h.2.mlp.act NewGELUActivation\n",
      "transformer.h.2.mlp.dropout Dropout\n",
      "transformer.h.3 GPT2Block\n",
      "transformer.h.3.ln_1 LayerNorm\n",
      "transformer.h.3.attn GPT2Attention\n",
      "transformer.h.3.attn.c_attn Conv1D\n",
      "transformer.h.3.attn.c_proj Conv1D\n",
      "transformer.h.3.attn.attn_dropout Dropout\n",
      "transformer.h.3.attn.resid_dropout Dropout\n",
      "transformer.h.3.ln_2 LayerNorm\n",
      "transformer.h.3.mlp GPT2MLP\n",
      "transformer.h.3.mlp.c_fc Conv1D\n",
      "transformer.h.3.mlp.c_proj Conv1D\n",
      "transformer.h.3.mlp.act NewGELUActivation\n",
      "transformer.h.3.mlp.dropout Dropout\n",
      "transformer.h.4 GPT2Block\n",
      "transformer.h.4.ln_1 LayerNorm\n",
      "transformer.h.4.attn GPT2Attention\n",
      "transformer.h.4.attn.c_attn Conv1D\n",
      "transformer.h.4.attn.c_proj Conv1D\n",
      "transformer.h.4.attn.attn_dropout Dropout\n",
      "transformer.h.4.attn.resid_dropout Dropout\n",
      "transformer.h.4.ln_2 LayerNorm\n",
      "transformer.h.4.mlp GPT2MLP\n",
      "transformer.h.4.mlp.c_fc Conv1D\n",
      "transformer.h.4.mlp.c_proj Conv1D\n",
      "transformer.h.4.mlp.act NewGELUActivation\n",
      "transformer.h.4.mlp.dropout Dropout\n",
      "transformer.h.5 GPT2Block\n",
      "transformer.h.5.ln_1 LayerNorm\n",
      "transformer.h.5.attn GPT2Attention\n",
      "transformer.h.5.attn.c_attn Conv1D\n",
      "transformer.h.5.attn.c_proj Conv1D\n",
      "transformer.h.5.attn.attn_dropout Dropout\n",
      "transformer.h.5.attn.resid_dropout Dropout\n",
      "transformer.h.5.ln_2 LayerNorm\n",
      "transformer.h.5.mlp GPT2MLP\n",
      "transformer.h.5.mlp.c_fc Conv1D\n",
      "transformer.h.5.mlp.c_proj Conv1D\n",
      "transformer.h.5.mlp.act NewGELUActivation\n",
      "transformer.h.5.mlp.dropout Dropout\n",
      "transformer.h.6 GPT2Block\n",
      "transformer.h.6.ln_1 LayerNorm\n",
      "transformer.h.6.attn GPT2Attention\n",
      "transformer.h.6.attn.c_attn Conv1D\n",
      "transformer.h.6.attn.c_proj Conv1D\n",
      "transformer.h.6.attn.attn_dropout Dropout\n",
      "transformer.h.6.attn.resid_dropout Dropout\n",
      "transformer.h.6.ln_2 LayerNorm\n",
      "transformer.h.6.mlp GPT2MLP\n",
      "transformer.h.6.mlp.c_fc Conv1D\n",
      "transformer.h.6.mlp.c_proj Conv1D\n",
      "transformer.h.6.mlp.act NewGELUActivation\n",
      "transformer.h.6.mlp.dropout Dropout\n",
      "transformer.h.7 GPT2Block\n",
      "transformer.h.7.ln_1 LayerNorm\n",
      "transformer.h.7.attn GPT2Attention\n",
      "transformer.h.7.attn.c_attn Conv1D\n",
      "transformer.h.7.attn.c_proj Conv1D\n",
      "transformer.h.7.attn.attn_dropout Dropout\n",
      "transformer.h.7.attn.resid_dropout Dropout\n",
      "transformer.h.7.ln_2 LayerNorm\n",
      "transformer.h.7.mlp GPT2MLP\n",
      "transformer.h.7.mlp.c_fc Conv1D\n",
      "transformer.h.7.mlp.c_proj Conv1D\n",
      "transformer.h.7.mlp.act NewGELUActivation\n",
      "transformer.h.7.mlp.dropout Dropout\n",
      "transformer.h.8 GPT2Block\n",
      "transformer.h.8.ln_1 LayerNorm\n",
      "transformer.h.8.attn GPT2Attention\n",
      "transformer.h.8.attn.c_attn Conv1D\n",
      "transformer.h.8.attn.c_proj Conv1D\n",
      "transformer.h.8.attn.attn_dropout Dropout\n",
      "transformer.h.8.attn.resid_dropout Dropout\n",
      "transformer.h.8.ln_2 LayerNorm\n",
      "transformer.h.8.mlp GPT2MLP\n",
      "transformer.h.8.mlp.c_fc Conv1D\n",
      "transformer.h.8.mlp.c_proj Conv1D\n",
      "transformer.h.8.mlp.act NewGELUActivation\n",
      "transformer.h.8.mlp.dropout Dropout\n",
      "transformer.h.9 GPT2Block\n",
      "transformer.h.9.ln_1 LayerNorm\n",
      "transformer.h.9.attn GPT2Attention\n",
      "transformer.h.9.attn.c_attn Conv1D\n",
      "transformer.h.9.attn.c_proj Conv1D\n",
      "transformer.h.9.attn.attn_dropout Dropout\n",
      "transformer.h.9.attn.resid_dropout Dropout\n",
      "transformer.h.9.ln_2 LayerNorm\n",
      "transformer.h.9.mlp GPT2MLP\n",
      "transformer.h.9.mlp.c_fc Conv1D\n",
      "transformer.h.9.mlp.c_proj Conv1D\n",
      "transformer.h.9.mlp.act NewGELUActivation\n",
      "transformer.h.9.mlp.dropout Dropout\n",
      "transformer.h.10 GPT2Block\n",
      "transformer.h.10.ln_1 LayerNorm\n",
      "transformer.h.10.attn GPT2Attention\n",
      "transformer.h.10.attn.c_attn Conv1D\n",
      "transformer.h.10.attn.c_proj Conv1D\n",
      "transformer.h.10.attn.attn_dropout Dropout\n",
      "transformer.h.10.attn.resid_dropout Dropout\n",
      "transformer.h.10.ln_2 LayerNorm\n",
      "transformer.h.10.mlp GPT2MLP\n",
      "transformer.h.10.mlp.c_fc Conv1D\n",
      "transformer.h.10.mlp.c_proj Conv1D\n",
      "transformer.h.10.mlp.act NewGELUActivation\n",
      "transformer.h.10.mlp.dropout Dropout\n",
      "transformer.h.11 GPT2Block\n",
      "transformer.h.11.ln_1 LayerNorm\n",
      "transformer.h.11.attn GPT2Attention\n",
      "transformer.h.11.attn.c_attn Conv1D\n",
      "transformer.h.11.attn.c_proj Conv1D\n",
      "transformer.h.11.attn.attn_dropout Dropout\n",
      "transformer.h.11.attn.resid_dropout Dropout\n",
      "transformer.h.11.ln_2 LayerNorm\n",
      "transformer.h.11.mlp GPT2MLP\n",
      "transformer.h.11.mlp.c_fc Conv1D\n",
      "transformer.h.11.mlp.c_proj Conv1D\n",
      "transformer.h.11.mlp.act NewGELUActivation\n",
      "transformer.h.11.mlp.dropout Dropout\n",
      "transformer.ln_f LayerNorm\n",
      "lm_head Linear\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "class LoRAConv1D(nn.Module):\n",
    "    def __init__(self, weight, bias, r, alpha):\n",
    "        super(LoRAConv1D, self).__init__()\n",
    "        self.nf, self.nx = weight.shape \n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = bias\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, self.nx))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(self.nf, self.r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        low_rank = self.B @ self.A\n",
    "        result += x.view(-1, x.size(-1)) @ low_rank\n",
    "        x = x.view(size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "------\n",
      "torch.Size([1892, 2304])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([1892, 2304])\n",
      "torch.Size([1892, 2304])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D):\n",
    "        print(module.nf)\n",
    "        print(module.weight.shape)\n",
    "        print(module.bias.shape)\n",
    "        break\n",
    "\n",
    "print(\"------\")\n",
    "weight = torch.randn([768, 2304])\n",
    "x = torch.randn([4,473,768])\n",
    "size_out = x.size()[:-1] + (2304, )\n",
    "A = torch.randn([8, 2304])\n",
    "B = torch.randn([768, 8])\n",
    "result_1 = x.view(-1, x.size(-1)) @ weight\n",
    "result_2 = B @ A\n",
    "print((result_1).shape)\n",
    "print((result_2).shape)\n",
    "result_2 = x.view(-1, x.size(-1)) @ result_2\n",
    "print((result_2).shape)\n",
    "print((result_1 + result_2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 8\n",
    "alpha = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D):\n",
    "        lora_layer = LoRAConv1D(module.weight, module.bias, r, alpha)\n",
    "        #lora_layer = LoRAConv1D(module.nf, module.weight.shape[0])\n",
    "        # Replace the module directly in the parent's _modules dictionary\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = dict(model.named_modules())[parent_name]\n",
    "        parent_module._modules[child_name] = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): LoRAConv1D()\n",
       "          (c_proj): LoRAConv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): LoRAConv1D()\n",
       "          (c_proj): LoRAConv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 9.75 GiB of which 18.19 MiB is free. Including non-PyTorch memory, this process has 9.63 GiB memory in use. Of the allocated memory 8.82 GiB is allocated by PyTorch, and 569.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m encodings \u001b[38;5;241m=\u001b[39m tokenizer(texts, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming encodings is your BatchEncoding object from tokenizer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/transformers/modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         )\n\u001b[0;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 9.75 GiB of which 18.19 MiB is free. Including non-PyTorch memory, this process has 9.63 GiB memory in use. Of the allocated memory 8.82 GiB is allocated by PyTorch, and 569.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "texts = dataset['train']['text'][:500]  # Directly access the text data\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Prepare data for training\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
