{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, weight, r, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        out_features = self.weight.shape[0]\n",
    "        in_features = self.weight.shape[1]\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, in_features))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(out_features, r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = x @ self.weight.T\n",
    "        result += x @ (self.A.T @ self.B.T)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e11914972/dl-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.15251220762729645\n",
      "Epoch 20, Loss: 0.07886414974927902\n",
      "Epoch 30, Loss: 0.08194424957036972\n",
      "Epoch 40, Loss: 0.08549709618091583\n",
      "Epoch 50, Loss: 0.0413505844771862\n",
      "Epoch 60, Loss: 0.03277064487338066\n",
      "Epoch 70, Loss: 0.009126568213105202\n",
      "Epoch 80, Loss: 0.006150983739644289\n",
      "Epoch 90, Loss: 0.0108120022341609\n",
      "Epoch 100, Loss: 0.008169732056558132\n",
      "Epoch 110, Loss: 0.002343089086934924\n",
      "Epoch 120, Loss: 0.005173786543309689\n",
      "Epoch 130, Loss: 0.0034724685829132795\n",
      "Epoch 140, Loss: 0.00361334509216249\n",
      "Epoch 150, Loss: 0.001160691026598215\n",
      "Epoch 160, Loss: 0.002678877441212535\n",
      "Epoch 170, Loss: 0.0018410688498988748\n",
      "Epoch 180, Loss: 0.0009976484579965472\n",
      "Epoch 190, Loss: 0.0008719780016690493\n",
      "Epoch 200, Loss: 0.001283199992030859\n",
      "Epoch 210, Loss: 0.0011635959381237626\n",
      "Epoch 220, Loss: 0.0005112708895467222\n",
      "Epoch 230, Loss: 0.00046528075472451746\n",
      "Epoch 240, Loss: 0.00106926285661757\n",
      "Epoch 250, Loss: 0.00039106555050238967\n",
      "Epoch 260, Loss: 0.0004478039045352489\n",
      "Epoch 270, Loss: 0.0006673947791568935\n",
      "Epoch 280, Loss: 0.00038123459671624005\n",
      "Epoch 290, Loss: 0.0005740281194448471\n",
      "Epoch 300, Loss: 0.0005343924858607352\n",
      "Epoch 310, Loss: 0.00024801649851724505\n",
      "Epoch 320, Loss: 0.00028416543500497937\n",
      "Epoch 330, Loss: 0.00026608144980855286\n",
      "Epoch 340, Loss: 0.0005023967241868377\n",
      "Epoch 350, Loss: 0.0003808822075370699\n",
      "Epoch 360, Loss: 0.0003615554596763104\n",
      "Epoch 370, Loss: 0.0004156489740125835\n",
      "Epoch 380, Loss: 0.00019682192942127585\n",
      "Epoch 390, Loss: 0.00037107348907738924\n",
      "Epoch 400, Loss: 0.00014328710676636547\n",
      "Input: [[1. 1.]], Predicted: [[0.01677224]]\n",
      "Input: [[0. 0.]], Predicted: [[0.01195995]]\n",
      "Input: [[0. 1.]], Predicted: [[0.98678106]]\n",
      "Input: [[1. 0.]], Predicted: [[0.98131394]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "ffn = FFN(2, 16, 1)\n",
    "x_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_xor)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "def train_xor_model(model, dataloader):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(400):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "def validate_xor_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for predictions\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            print(f\"Input: {inputs.numpy()}, Predicted: {outputs.numpy()}\")\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.31432613730430603\n",
      "Epoch 20, Loss: 0.004087598063051701\n",
      "Epoch 30, Loss: 0.9512374401092529\n",
      "Epoch 40, Loss: 0.26251086592674255\n",
      "Epoch 50, Loss: 0.25592029094696045\n",
      "Epoch 60, Loss: 0.23843255639076233\n",
      "Epoch 70, Loss: 7.103406460373662e-06\n",
      "Epoch 80, Loss: 0.01003202609717846\n",
      "Epoch 90, Loss: 0.008165196515619755\n",
      "Epoch 100, Loss: 7.21248215995729e-06\n",
      "Epoch 110, Loss: 0.00016673911886755377\n",
      "Epoch 120, Loss: 0.0001731382799334824\n",
      "Epoch 130, Loss: 0.004492830950766802\n",
      "Epoch 140, Loss: 0.07628859579563141\n",
      "Epoch 150, Loss: 0.00018565756909083575\n",
      "Epoch 160, Loss: 0.003221475752070546\n",
      "Epoch 170, Loss: 0.053204506635665894\n",
      "Epoch 180, Loss: 8.907381925382651e-06\n",
      "Epoch 190, Loss: 0.00019771434017457068\n",
      "Epoch 200, Loss: 9.146630873146933e-06\n",
      "Epoch 210, Loss: 0.002004998968914151\n",
      "Epoch 220, Loss: 0.00020425960246939212\n",
      "Epoch 230, Loss: 0.02937709540128708\n",
      "Epoch 240, Loss: 9.550826689519454e-06\n",
      "Epoch 250, Loss: 0.0014442935353145003\n",
      "Epoch 260, Loss: 9.695789231045637e-06\n",
      "Epoch 270, Loss: 0.0012509647058323026\n",
      "Epoch 280, Loss: 0.00020698781008832157\n",
      "Epoch 290, Loss: 9.872908776742406e-06\n",
      "Epoch 300, Loss: 0.0010134001495316625\n",
      "Epoch 310, Loss: 0.0009461329900659621\n",
      "Epoch 320, Loss: 0.0008890170720405877\n",
      "Epoch 330, Loss: 0.014181075617671013\n",
      "Epoch 340, Loss: 9.999918802350294e-06\n",
      "Epoch 350, Loss: 0.00020317916641943157\n",
      "Epoch 360, Loss: 0.0006965273059904575\n",
      "Epoch 370, Loss: 0.011205914430320263\n",
      "Epoch 380, Loss: 0.00020011521701235324\n",
      "Epoch 390, Loss: 0.010045479983091354\n",
      "Epoch 400, Loss: 0.009526100009679794\n",
      "Input: [[0. 1.]], Predicted: [[0.99683595]]\n",
      "Input: [[1. 0.]], Predicted: [[0.98594856]]\n",
      "Input: [[0. 0.]], Predicted: [[0.09753636]]\n",
      "Input: [[1. 1.]], Predicted: [[0.97648126]]\n"
     ]
    }
   ],
   "source": [
    "ffn_weight = ffn.linear1.weight.detach().clone()\n",
    "lora_layer = LoRALayer(ffn_weight, 1, 0.1)\n",
    "setattr(ffn, 'linear1', lora_layer)\n",
    "\n",
    "y_or = torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_or)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of FFN(\n",
       "  (linear1): LoRALayer()\n",
       "  (linear2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "class LoRAConv1D(nn.Module):\n",
    "    def __init__(self, weight, bias, r, alpha):\n",
    "        super(LoRAConv1D, self).__init__()\n",
    "        self.nf, self.nx = weight.shape \n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = bias\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, self.nx))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(self.nf, self.r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        low_rank = self.B @ self.A\n",
    "        result += x.view(-1, x.size(-1)) @ low_rank\n",
    "        x = x.view(size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.1.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.2.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.3.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.4.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.5.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.6.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.7.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.8.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.9.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.10.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "transformer.h.11.attn.c_attn\n",
      "2304\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "------\n",
      "torch.Size([1892, 2304])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([1892, 2304])\n",
      "torch.Size([1892, 2304])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D) and \"c_attn\" in str(name):\n",
    "        print(name)\n",
    "        print(module.nf)\n",
    "        print(module.weight.shape)\n",
    "        print(module.bias.shape)\n",
    "    \n",
    "\n",
    "print(\"------\")\n",
    "weight = torch.randn([768, 2304])\n",
    "x = torch.randn([4,473,768])\n",
    "size_out = x.size()[:-1] + (2304, )\n",
    "A = torch.randn([8, 2304])\n",
    "B = torch.randn([768, 8])\n",
    "result_1 = x.view(-1, x.size(-1)) @ weight\n",
    "result_2 = B @ A\n",
    "print((result_1).shape)\n",
    "print((result_2).shape)\n",
    "result_2 = x.view(-1, x.size(-1)) @ result_2\n",
    "print((result_2).shape)\n",
    "print((result_1 + result_2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all the attention layers in model with LoRA layers\n",
    "r = 8\n",
    "alpha = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D) and \"c_attn\" in str(name):\n",
    "        lora_layer = LoRAConv1D(module.weight, module.bias, r, alpha)\n",
    "        # Replace the module directly in the parent's _modules dictionary\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = dict(model.named_modules())[parent_name]\n",
    "        parent_module._modules[child_name] = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): LoRAConv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv1D):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"attn.c_attn\" in name: assert param.requires_grad == True\n",
    "    else: assert param.requires_grad == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e11914972/dl-env/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/e11914972/dl-env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/48 [00:00<?, ?it/s]/home/e11914972/dl-env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://hub.jaas.datalab.tuwien.ac.at/user/e11914972/'. Verify the server is running and reachable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import tqdm\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "texts = dataset['train']['text'][:500]  # Using a small slice for quick training\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare data for training\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader)*3)\n",
    "\n",
    "# Setup for mixed-precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "progress_bar = tqdm.tqdm(range(len(dataloader) * 3), desc=\"Training\")\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.50M/3.50M [00:00<00:00, 13.8MB/s]\n",
      "Downloading data: 100%|██████████| 408k/408k [00:00<00:00, 2.52MB/s]\n",
      "Downloading data: 100%|██████████| 76.5k/76.5k [00:00<00:00, 517kB/s]\n",
      "Downloading data: 100%|██████████| 47.5M/47.5M [00:00<00:00, 110MB/s] \n",
      "Generating test split: 100%|██████████| 14042/14042 [00:00<00:00, 768671.99 examples/s]\n",
      "Generating validation split: 100%|██████████| 1531/1531 [00:00<00:00, 557759.00 examples/s]\n",
      "Generating dev split: 100%|██████████| 285/285 [00:00<00:00, 191015.76 examples/s]\n",
      "Generating auxiliary_train split: 100%|██████████| 99842/99842 [00:00<00:00, 386056.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "dataset = load_dataset(\"cais/mmlu\")\n",
    "choices = ['A', 'B', 'C', 'D']\n",
    "\n",
    "def format_subject(subject):\n",
    "    return ' '.join(subject.split('_'))\n",
    "\n",
    "def format_example(df, idx, choices, include_answer=True):\n",
    "    prompt = df['question'][idx]\n",
    "    for j, choice in enumerate(choices):\n",
    "        prompt += f\"\\n{j+1}. {df[choice][idx]}\"\n",
    "    if include_answer:\n",
    "        prompt += f\"\\nAnswer: {df['correct_answer'][idx]}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(df, subject, n_examples=-1):\n",
    "    subject_formatted = format_subject(subject)\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {subject_formatted}.\\n\\n\"\n",
    "    max_examples = df.shape[0] if n_examples == -1 else n_examples\n",
    "    for i in range(max_examples):\n",
    "        prompt += format_example(df, i, ['choice1', 'choice2', 'choice3', 'choice4'], include_answer=True)\n",
    "    return prompt\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dev_df, test_df, subject, num_train_examples):\n",
    "    cors = []\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        train_prompt = gen_prompt(dev_df, subject, num_train_examples)\n",
    "        prompt_end = format_example(test_df, i, ['choice1', 'choice2', 'choice3', 'choice4'], include_answer=False)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        \n",
    "        # Tokenize and ensure input length is within the model's limits\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "        \n",
    "        # Generate logits\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        probs = (\n",
    "            F.softmax(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        logits[tokenizer(\"A\").input_ids[0]],\n",
    "                        logits[tokenizer(\"B\").input_ids[0]],\n",
    "                        logits[tokenizer(\"C\").input_ids[0]],\n",
    "                        logits[tokenizer(\"D\").input_ids[0]],\n",
    "                    ]\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "        pred = np.argmax(probs)\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        correct = test_df['correct_answer'][i]\n",
    "        cor = pred == correct\n",
    "        cors.append(cor)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    accuracy = np.mean(cors)\n",
    "    print(f\"Average accuracy: {accuracy:.3f} - Subject: {subject}\")\n",
    "\n",
    "    return np.array(cors), accuracy, np.array(all_probs)\n",
    "\n",
    "# Example usage\n",
    "dev_df = dataset['validation']\n",
    "test_df = dataset['test']\n",
    "subject = 'subject_name_here'  # Replace with actual subject\n",
    "num_train_examples = 5  # Number of training examples to include in each prompt\n",
    "\n",
    "# Evaluate model\n",
    "results = evaluate(model, tokenizer, dev_df, test_df, subject, num_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
