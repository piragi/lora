{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import Conv1D\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRAConv1D(nn.Module):\n",
    "    def __init__(self, weight, bias, r, alpha):\n",
    "        super(LoRAConv1D, self).__init__()\n",
    "        self.nx, self.nf = weight.shape \n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = bias\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, self.nx))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(self.nf, self.r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        low_rank = self.B @ self.A\n",
    "        result += x.view(-1, x.size(-1)) @ low_rank.T\n",
    "        result = result.view(size_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all the attention layers in model with LoRA layers\n",
    "r = 64\n",
    "alpha = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D) and \"c_attn\" in str(name):\n",
    "        lora_layer = LoRAConv1D(module.weight, module.bias, r, alpha)\n",
    "        # Replace the module directly in the parent's _modules dictionary\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = dict(model.named_modules())[parent_name]\n",
    "        parent_module._modules[child_name] = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): LoRAConv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn.c_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name: \u001b[38;5;28;01massert\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01massert\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv1D):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"attn.c_attn\" in name: assert param.requires_grad == True\n",
    "    else: assert param.requires_grad == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=0\n",
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if \"attn.c_\" in name:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"attn.c_\" in name: assert param.requires_grad == True\n",
    "    else: assert param.requires_grad == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model with total params: 28348416 for r= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/11475 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training: 100%|██████████| 11475/11475 [37:03<00:00,  5.16it/s, loss=0.478] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.47788745164871216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import tqdm\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "texts = dataset['train']['text']  # Using a small slice for quick training\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'load model with total params: {pytorch_total_params} for r= {r}')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare data for training\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader)*5)\n",
    "\n",
    "# Setup for mixed-precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "progress_bar = tqdm.tqdm(range(len(dataloader) * 5), desc=\"Training\")\n",
    "for epoch in range(5):  # 5 epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"./gpt2_r{r}_16b_512.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load gpt2_r64_16b_512.pt with total params: 23620608 for r=64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa54f5d3b9a48d2b4814127573f2800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.2929913997650146\n",
      "load gpt2_r4_16b_512.pt with total params: 21408768 for r=4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695b72e27aef493cb3070699aa6aafef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.297861099243164\n",
      "load gpt2_r2_16b_512.pt with total params: 21335040 for r=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b950502ea1ad46ed9f9b815b8b537a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.2926536798477173\n",
      "load gpt2_r0_16b_512.pt with total params: 28348416 for r=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5eb94604a44ae0bd6789281ba2bdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.2891448736190796\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm  # Use tqdm.auto for a progress bar that automatically adjusts to the environment\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, batch_size=16):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_length = 0\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    progress_bar = tqdm(data_loader, desc=\"Computing Perplexity\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024)\n",
    "        input_ids = inputs.input_ids.to(model.device)\n",
    "        attention_mask = inputs.attention_mask.to(model.device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            num_tokens = attention_mask.sum()  # Only count tokens that are not padding\n",
    "            total_loss += loss.item() * num_tokens.item()  # Adjust for the actual number of tokens\n",
    "            total_length += num_tokens.item()\n",
    "\n",
    "        if total_length > 0:\n",
    "            current_perplexity = torch.exp(torch.tensor(total_loss / total_length)).item()\n",
    "        else:\n",
    "            current_perplexity = float('inf')  # Handle case where total_length is zero\n",
    "        progress_bar.set_postfix({'current perplexity': current_perplexity})\n",
    "\n",
    "    if total_length > 0:\n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_length)).item()\n",
    "    else:\n",
    "        perplexity = float('inf')  # Handle case where total_length is zero\n",
    "    return perplexity\n",
    "\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the WikiText validation dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "\n",
    "models_r = [64, 4, 2, 0]\n",
    "for r in models_r:\n",
    "    model_name = f'gpt2_r{r}_16b_512.pt'\n",
    "    model = torch.load(f'./{model_name}')\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'load {model_name} with total params: {pytorch_total_params} for r={r}')    \n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    perplexity = compute_perplexity(model, tokenizer, dataset)\n",
    "    print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b754937ced7c4544861415f7c93a8587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17740.435546875\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "perplexity = compute_perplexity(model, tokenizer, dataset)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", 'all')\n",
    "choices = ['A', 'B', 'C', 'D']\n",
    "subjects = ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', \n",
    "            'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', \n",
    "            'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', \n",
    "            'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', \n",
    "            'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', \n",
    "            'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', \n",
    "            'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', \n",
    "            'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', \n",
    "            'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', \n",
    "            'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', \n",
    "            'virology', 'world_religions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9195e+01, -8.8060e+01, -6.6721e+01, -7.6704e+01, -7.8159e+01,\n",
      "         -9.2187e+01, -8.8048e+01, -7.5561e+01, -1.1095e+02, -1.1478e+02,\n",
      "         -8.3513e+01, -7.6295e+01, -7.1059e+01, -1.0340e+02, -1.5245e+02,\n",
      "          2.8419e+01, -4.8414e+01, -7.0397e+01, -6.6105e+01, -7.1978e+01,\n",
      "         -7.9528e+01, -7.0824e+01, -8.9900e+01, -2.6417e+01, -7.7344e+01,\n",
      "         -1.8183e+00, -5.4304e+01, -7.5323e+01, -9.9255e+01, -8.1886e+01,\n",
      "         -8.3881e+01, -4.8993e+01, -7.6000e+01, -7.2275e+01, -7.8384e+01,\n",
      "         -6.9850e+01, -7.9477e+01, -7.1126e+01, -9.4997e+01, -1.0155e+02,\n",
      "         -9.5762e+01, -9.7707e+01, -4.7966e+01, -7.4537e+01, -8.7895e+01,\n",
      "         -6.5548e+01, -4.2067e+01, -1.0200e+02, -8.2504e+01,  5.1081e+01,\n",
      "         -1.9441e+01, -2.0478e+01, -7.1574e+01,  1.5196e+01, -5.6091e+01,\n",
      "          8.5105e+01, -9.8095e+01,  1.7449e+01, -7.8288e+01, -9.2535e+01,\n",
      "         -9.8119e+01, -6.3177e+01, -3.0487e+01, -6.4403e+01, -8.4969e+01,\n",
      "         -8.6147e+01, -6.1624e+01, -7.4469e+01, -5.9805e+01, -9.0236e+01,\n",
      "         -7.0492e+01, -8.9827e+01, -9.1038e+01, -7.2800e+01, -7.3292e+01,\n",
      "         -6.5725e+01, -9.5839e+01, -9.1587e+01, -1.0518e+02, -9.5680e+01,\n",
      "         -9.0652e+01, -2.3439e+01, -1.1188e+01, -7.4729e+01, -7.8221e+01,\n",
      "         -7.1645e+01, -4.2735e+01, -9.9931e+01, -7.8432e+01, -2.9349e+01,\n",
      "         -1.5997e+01, -9.6705e+01, -4.5138e+01, -3.6725e+01, -8.0658e+01,\n",
      "         -5.0781e+01, -7.8328e+01, -5.5011e+01, -4.4577e+01, -7.1193e+01,\n",
      "         -3.5667e+01, -5.0259e+01, -5.4956e+01, -6.7386e+01, -7.0574e+01,\n",
      "         -3.3634e+01, -1.1035e+01, -1.0215e+02, -6.1217e+01, -7.2296e+01,\n",
      "         -3.6890e+01,  2.2977e+01, -6.8446e+01, -2.5928e+01, -6.8905e+01,\n",
      "         -3.2085e+01, -2.7746e+01,  9.0284e+01, -6.0102e+01,  3.4613e+01,\n",
      "         -7.1765e+01,  1.0654e+01, -7.7829e+00,  6.8079e+01, -4.8659e+01,\n",
      "          5.3845e+00, -4.5856e+01, -1.1635e+01, -1.2517e+01, -2.2129e+02,\n",
      "         -4.7736e+01,  1.5332e+01, -3.5188e+01, -1.8997e+01, -3.6831e+01,\n",
      "         -5.7757e+00, -3.1916e+00, -1.3040e+01,  2.0802e+01, -2.7153e+00,\n",
      "         -6.8330e+01, -2.4657e+01,  3.2097e+01, -5.9926e+01, -9.9830e+01,\n",
      "         -6.2663e+01, -6.6170e+01, -3.7348e+01, -7.0986e+01, -8.0360e+01,\n",
      "         -1.0418e+02, -5.8024e+01, -8.9447e+01, -4.2979e+01, -1.0291e+02,\n",
      "         -9.4616e+01, -1.0333e+02, -1.1276e+02, -5.6847e+01, -1.3593e+01,\n",
      "          9.1989e+01, -1.1100e+02, -9.3192e+01, -5.3391e+01, -9.5506e+01,\n",
      "         -8.8370e+01, -6.7969e+01, -7.7632e+01, -8.2209e+01, -9.1644e+01,\n",
      "         -9.1492e+01, -7.2707e+01, -7.7744e+01, -4.1279e+01,  3.7417e+01,\n",
      "         -9.4413e+01,  4.1559e+01, -4.4020e+01,  1.6523e+01,  6.0258e+00,\n",
      "         -2.4836e+02, -3.6169e+01,  6.7705e+01,  5.8411e+01,  2.6400e+01,\n",
      "          3.2514e+01, -2.5130e+02, -1.4361e+01,  1.2061e+02, -7.9465e-02,\n",
      "          2.8794e+01,  5.3816e+01, -2.5988e+02, -9.1629e+00,  9.6680e+01,\n",
      "          7.0632e+01,  6.9232e+01,  4.7983e+01,  1.1596e+02,  8.6023e+01,\n",
      "          5.5970e+01,  1.0726e+02, -4.6655e+01, -3.4760e+01,  1.1369e+02,\n",
      "          1.0475e+02, -5.7951e+01, -8.1495e+01, -8.9521e+01, -6.9599e+01,\n",
      "         -6.9262e+01, -9.1817e+01, -6.5752e+01, -4.4712e+01, -8.7348e+01,\n",
      "         -1.0551e+02, -1.0565e+02, -8.5174e+01, -1.1122e+02, -9.4735e+01,\n",
      "         -4.1528e+01,  1.0671e+02, -1.1330e+02, -9.3974e+01, -9.7781e+01,\n",
      "         -4.5334e+01, -8.1911e+01, -8.3568e+01, -5.0590e+01, -7.2628e+01,\n",
      "         -4.7866e+01, -1.0143e+02, -1.0519e+02, -8.8545e+01, -5.7521e+00,\n",
      "          8.2251e+00, -5.8878e+01, -2.5612e+01, -4.3589e+01,  1.0324e+02,\n",
      "         -4.3544e+01,  1.1573e+02,  2.1171e+01,  1.3474e+02,  6.9722e+01,\n",
      "          5.6384e+01, -8.2898e+00,  6.1623e+01,  7.0024e+01,  5.5966e+01,\n",
      "          2.7284e+01, -2.4970e+02, -3.7725e+00,  1.2987e+02,  5.3538e+01,\n",
      "          1.9935e+01, -6.1038e+01, -2.5729e+02,  1.9553e+01,  1.1953e+02,\n",
      "         -2.0254e+02,  9.0096e+01,  2.2437e+01, -2.4305e+02,  5.4890e+01,\n",
      "          4.7974e+01,  5.1881e+01, -2.3056e+01, -3.2146e+01, -3.4417e+01,\n",
      "         -1.0477e+02, -1.1220e+02, -6.9485e+01, -9.8871e+01, -5.7625e+01,\n",
      "         -7.2734e+01, -6.4296e+01, -4.3406e+01, -3.3958e+01,  3.1886e+01,\n",
      "         -6.6421e+01,  3.4223e+00,  2.2901e+00,  2.2930e+01, -7.5142e+01,\n",
      "          1.3653e+01,  1.3726e+01, -2.4345e+02, -1.1614e+02,  1.7082e+01,\n",
      "         -3.5969e+01, -2.5458e+02, -1.4366e+02,  6.0232e+01, -2.6670e+01,\n",
      "          1.1234e+02,  2.8316e+01, -2.3133e+01,  3.4128e+01, -3.0456e+01,\n",
      "         -2.9988e+01, -8.4026e+01, -3.5348e+01, -8.8062e+01, -3.7116e+01,\n",
      "         -8.3254e+01, -4.6941e+01,  5.2566e+01, -7.2044e+01, -1.0182e+02,\n",
      "         -6.4935e+01, -6.9156e+01, -9.1407e+01, -1.1461e+02, -7.2133e+01,\n",
      "         -4.3691e+01,  1.2803e+02, -5.1368e+01,  2.8723e+00,  5.7986e+00,\n",
      "          1.5779e+02, -1.0603e+02,  2.9805e+01,  2.9296e+00, -2.4860e+02,\n",
      "         -1.9069e+02,  1.5770e+02, -1.5190e+01, -2.5203e+02, -1.3281e+02,\n",
      "          1.4337e+01, -1.9788e+01,  7.7861e+01,  3.2195e+01]], device='cuda:0')\n",
      "torch.Size([1, 334, 50257])\n",
      "[[[0.27839825 0.2651024  0.31001642 ... 0.65083396 0.49672416 0.7285901 ]]\n",
      "\n",
      " [[0.2801112  0.44449502 0.3231935  ... 0.12380771 0.19847009 0.22111538]]\n",
      "\n",
      " [[0.22605142 0.13022086 0.1788661  ... 0.09685766 0.26737243 0.04043766]]\n",
      "\n",
      " [[0.21543911 0.16018173 0.18792401 ... 0.1285007  0.03743338 0.00985691]]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "228",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m num_train_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of training examples to include in each prompt\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_examples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 64\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer, dev_df, val_df, subject, num_train_examples)\u001b[0m\n\u001b[1;32m     47\u001b[0m probs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[1;32m     49\u001b[0m         torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(probs)\n\u001b[0;32m---> 64\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     65\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(probs)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Check if the prediction is correct\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 228"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def format_subject(subject):\n",
    "    return ' '.join(subject.split('_'))\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    entry = df[idx]\n",
    "    prompt = entry['question']\n",
    "    for j, choice in enumerate(entry['choices']):\n",
    "        prompt += f\"\\n{choices[j]}. {choice}\"\n",
    "    prompt += f\"\\nAnswer:\"\n",
    "    if include_answer:\n",
    "        prompt += f\"\\n{choices[entry['answer']]}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(df, subject, n_examples=-1):\n",
    "    subject_formatted = format_subject(subject)\n",
    "    df = df.filter(lambda x: x['subject'] == subject)\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {subject_formatted}.\\n\\n\"\n",
    "    max_examples = df.shape[0] if n_examples == -1 else n_examples\n",
    "    for i in range(max_examples):\n",
    "        prompt += format_example(df, i, include_answer=True)\n",
    "    return prompt\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dev_df, val_df, subject, num_train_examples):\n",
    "    cors = []\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(len(val_df)):\n",
    "        subject = val_df['subject'][i]\n",
    "        train_prompt = gen_prompt(dev_df, subject, num_train_examples)\n",
    "        prompt_end = format_example(val_df, i, include_answer=False)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        # Tokenize and ensure input length is within the model's limits\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "        \n",
    "        # Generate logits\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        print(logits[:, :, tokenizer('A').input_ids[0]])\n",
    "        print(logits.shape)\n",
    "        probs = (\n",
    "            F.softmax(\n",
    "                torch.stack(\n",
    "                    [\n",
    "                        logits[:, :, tokenizer(\"A\").input_ids[0]],\n",
    "                        logits[:, :, tokenizer(\"B\").input_ids[0]],\n",
    "                        logits[:, :, tokenizer(\"C\").input_ids[0]],\n",
    "                        logits[:, :, tokenizer(\"D\").input_ids[0]],\n",
    "                    ]\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        print(probs)\n",
    "        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "        pred = np.argmax(probs)\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        correct = val_df[i]['answer']\n",
    "        cor = pred == correct\n",
    "        cors.append(cor)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    accuracy = np.mean(cors)\n",
    "    print(f\"Average accuracy: {accuracy:.3f} - Subject: {subject}\")\n",
    "\n",
    "    return np.array(cors), accuracy, np.array(all_probs)\n",
    "\n",
    "# Example usage\n",
    "dev_df = dataset['dev']\n",
    "val_df = dataset['validation']\n",
    "subject = 'all_facts'  # Replace with actual subject\n",
    "num_train_examples = 5  # Number of training examples to include in each prompt\n",
    "\n",
    "# Evaluate model\n",
    "results = evaluate(model, tokenizer, dev_df, val_df, subject, num_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('D').input_ids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
