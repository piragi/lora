{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:27.203518Z",
     "start_time": "2024-06-25T18:46:26.651914Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = copy.deepcopy(bert)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                \n",
    "    def print_grad_list(self): \n",
    "        grad_list = [] \n",
    "        for p in self.bert.parameters():\n",
    "            grad_list.append(p.requires_grad)\n",
    "        print(grad_list)\n",
    "        \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.bert.parameters() if p.requires_grad)\n",
    "\n",
    "class LoRALayer(nn.Module):    \n",
    "    def __init__(self, layer, r = 2, alpha = 1.0):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(r).float())\n",
    "        self.A0 = nn.Parameter(torch.zeros(layer.in_features, self.r) * std_dev)\n",
    "        self.B0 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A1 = nn.Parameter(torch.randn(layer.in_features, self.r) * std_dev)\n",
    "        self.B1 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A2 = nn.Parameter(torch.randn(layer.in_features, self.r) * std_dev)\n",
    "        self.B2 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A = self.A1\n",
    "        self.B = self.B1\n",
    "        self.A0.requires_grad = False\n",
    "        self.B0.requires_grad = False\n",
    "        self.A1.requires_grad = True\n",
    "        self.B1.requires_grad = True\n",
    "        self.A2.requires_grad = False\n",
    "        self.B2.requires_grad = False\n",
    "        \n",
    "    def switch_task(self, task_id):\n",
    "        if task_id == 0:\n",
    "            self.A = self.A0\n",
    "            self.B = self.B0\n",
    "        elif task_id == 1:\n",
    "            self.A = self.A1\n",
    "            self.B = self.B1\n",
    "            self.A1.requires_grad = True\n",
    "            self.B1.requires_grad = True\n",
    "            self.A2.requires_grad = False\n",
    "            self.B2.requires_grad = False\n",
    "        elif task_id == 2:  # Corrected to task_id == 2\n",
    "            self.A = self.A2\n",
    "            self.B = self.B2\n",
    "            self.A1.requires_grad = False\n",
    "            self.B1.requires_grad = False\n",
    "            self.A2.requires_grad = True\n",
    "            self.B2.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.layer(x) + self.alpha * (x @ self.A @ self.B)\n",
    "        return result\n",
    "\n",
    "class BertWithSwitchableTask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertWithSwitchableTask, self).__init__()\n",
    "        self.bert = copy.deepcopy(bert)\n",
    "        self.lora_layers = []\n",
    "        self.add_lora_layers()\n",
    "\n",
    "    def switch_task(self, task_id):\n",
    "        for layer in self.lora_layers:\n",
    "            layer.switch_task(task_id)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        result = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return result\n",
    "\n",
    "    def add_lora_layers(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for layer in self.bert.distilbert.transformer.layer:\n",
    "                layer.attention.q_lin = LoRALayer(layer.attention.q_lin)\n",
    "                layer.attention.k_lin = LoRALayer(layer.attention.k_lin)\n",
    "                layer.attention.v_lin = LoRALayer(layer.attention.v_lin)\n",
    "                layer.attention.out_lin = LoRALayer(layer.attention.out_lin)\n",
    "                layer.ffn.lin1 = LoRALayer(layer.ffn.lin1)\n",
    "                layer.ffn.lin2 = LoRALayer(layer.ffn.lin2)\n",
    "                self.lora_layers += [\n",
    "                    layer.attention.q_lin,\n",
    "                    layer.attention.k_lin,\n",
    "                    layer.attention.v_lin,\n",
    "                    layer.attention.out_lin,\n",
    "                    layer.ffn.lin1,\n",
    "                    layer.ffn.lin2,\n",
    "                ]\n",
    "        self.bert.pre_classifier = LoRALayer(self.bert.pre_classifier)\n",
    "        self.bert.classifier = LoRALayer(self.bert.classifier)\n",
    "        self.lora_layers += [\n",
    "            self.bert.pre_classifier,\n",
    "            self.bert.classifier,\n",
    "        ]\n",
    "                \n",
    "    def print_grad_list(self): \n",
    "        grad_list = [] \n",
    "        for p in self.bert.parameters():\n",
    "            grad_list.append(p.requires_grad)\n",
    "        print(grad_list)\n",
    "        \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.bert.parameters() if p.requires_grad)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:27.405717Z",
     "start_time": "2024-06-25T18:46:27.204477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BertWithSwitchableTask().print_grad_list()\n",
    "print(f\"TRAINABLE PARAMS FOR MODEL: {Bert().count_trainable_parameters()}\")\n",
    "print(f\"TRAINABLE PARAMS FOR LORA MODEL: {BertWithSwitchableTask().count_trainable_parameters()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False]\n",
      "TRAINABLE PARAMS FOR MODEL: 66958086\n",
      "TRAINABLE PARAMS FOR LORA MODEL: 170508\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:27.410154Z",
     "start_time": "2024-06-25T18:46:27.406478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class CustomDataloader:\n",
    "    def __init__(self, name, label, prct):\n",
    "        self.label = label\n",
    "        \n",
    "        # Load the dataset\n",
    "        news_dataset = load_dataset(name, split=f\"train[:{prct}%]\")  # Using 1% of the dataset for a quick demonstration\n",
    "        news_dataset = news_dataset.train_test_split(test_size=0.2)  # Split the dataset into train and test sets\n",
    "            \n",
    "        train_dataset = news_dataset['train']\n",
    "        test_dataset = news_dataset['test']\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label])\n",
    "        test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label])\n",
    "        \n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "        \n",
    "    def train(self):\n",
    "        return self.train_dataloader\n",
    "        \n",
    "    def test(self):\n",
    "        return self.test_dataloader"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:27.416576Z",
     "start_time": "2024-06-25T18:46:27.411158Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\"ag_news\"\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, dataloader: CustomDataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def train_model(self, model, name):\n",
    "        model.to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            progress_bar = tqdm.tqdm(self.dataloader.train(), desc=f\"Training\")\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                inputs = {\"input_ids\": batch[\"input_ids\"].to(device), \"attention_mask\": batch[\"attention_mask\"].to(device), \"labels\": batch[self.dataloader.label].to(device)}\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs.logits, batch[self.dataloader.label].to(device))\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_postfix(epoch=epoch, loss=loss.item())\n",
    "            print(f\"Eval accuracy = {self.eval_model(model)}\")\n",
    "            \n",
    "        \n",
    "        torch.save(model, f\"./models/{name}.pt\")\n",
    "        return model\n",
    "        \n",
    "    def eval_model(self, model):\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.dataloader.test():\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch[self.dataloader.label].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_predictions += len(labels)\n",
    "        \n",
    "        eval_accuracy = correct_predictions / total_predictions\n",
    "        return eval_accuracy"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:31.045727Z",
     "start_time": "2024-06-25T18:46:27.417457Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader_news = CustomDataloader(\"ag_news\", \"label\", 1)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/960 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "157bf3b6dd9d45e9ae6d4353c6fbea53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd1243e2c0e6445d9cb30d1266af6672"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:33.749497Z",
     "start_time": "2024-06-25T18:46:31.046752Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader_trec = CustomDataloader(\"trec\", \"coarse_label\", 10)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/436 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bff0558e7374cd4b1803bb4a6a46b3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/109 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30b0576a3bd940dfa85a9338376fdb7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:46:33.866146Z",
     "start_time": "2024-06-25T18:46:33.750747Z"
    }
   },
   "source": "bert_switchable = BertWithSwitchableTask().to(device)",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:47:48.926779Z",
     "start_time": "2024-06-25T18:46:33.867102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(1)\n",
    "bert_switchable = Trainer(dataloader_trec).train_model(bert_switchable, \"distilbert_lora\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 218/218 [00:13<00:00, 15.91it/s, epoch=0, loss=1.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.6238532110091743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 218/218 [00:13<00:00, 15.87it/s, epoch=1, loss=1.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.7706422018348624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 218/218 [00:13<00:00, 15.70it/s, epoch=2, loss=0.961] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8348623853211009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 218/218 [00:13<00:00, 15.71it/s, epoch=3, loss=0.0254] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8165137614678899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 218/218 [00:14<00:00, 15.52it/s, epoch=4, loss=0.589]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.7798165137614679\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:50:28.578672Z",
     "start_time": "2024-06-25T18:47:48.928039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(2)\n",
    "bert_switchable = Trainer(dataloader_news).train_model(bert_switchable, \"distilbert_lora\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 480/480 [00:29<00:00, 16.22it/s, epoch=0, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 480/480 [00:29<00:00, 16.24it/s, epoch=1, loss=0.251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.9083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 480/480 [00:29<00:00, 16.35it/s, epoch=2, loss=0.101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 480/480 [00:29<00:00, 16.12it/s, epoch=3, loss=1.4]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8791666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 480/480 [00:29<00:00, 16.19it/s, epoch=4, loss=0.12]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8708333333333333\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:50:38.338560Z",
     "start_time": "2024-06-25T18:50:28.580414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(0)\n",
    "print(f\"Accuracy on news dataset without switching: {Trainer(dataloader_news).eval_model(bert_switchable)}\")\n",
    "print(f\"Accuracy on trec dataset without switching: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "\n",
    "bert_switchable.switch_task(1)\n",
    "print(f\"Accuracy on ag_news dataset when switched to trec: {Trainer(dataloader_news).eval_model(bert_switchable)}\")\n",
    "bert_switchable.switch_task(2)\n",
    "print(f\"Accuracy on trec dataset when switched to ag_news: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "\n",
    "bert_switchable.switch_task(1)\n",
    "print(f\"Accuracy on trec dataset when switched to trec: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "bert_switchable.switch_task(2)\n",
    "print(f\"Accuracy on news dataset when switched to news: {Trainer(dataloader_news).eval_model(bert_switchable)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on news dataset without switching: 0.22083333333333333\n",
      "Accuracy on trec dataset without switching: 0.1926605504587156\n",
      "Accuracy on ag_news dataset when switched to trec: 0.1\n",
      "Accuracy on trec dataset when switched to ag_news: 0.10091743119266056\n",
      "Accuracy on trec dataset when switched to trec: 0.7798165137614679\n",
      "Accuracy on news dataset when switched to news: 0.8708333333333333\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:55:03.139509Z",
     "start_time": "2024-06-25T18:55:02.993069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "bert_switchable.switch_task(1)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to switch tasks on lora model: {end - start}\")\n",
    "\n",
    "start = time.process_time()\n",
    "test = torch.load(\"models/distilbert_lora.pt\").to(device)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to load another model from drive: {end - start}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to switch tasks on lora model: 0.0004307069999640589\n",
      "Time it takes to load another model from drive: 0.1443533159999788\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
