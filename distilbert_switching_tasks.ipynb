{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:34:13.770043Z",
     "start_time": "2024-06-25T22:34:13.210732Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = copy.deepcopy(bert)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.bert.parameters() if p.requires_grad)\n",
    "\n",
    "class LoRALayer(nn.Module):    \n",
    "    def __init__(self, layer, r = 2, alpha = 1.0):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(r).float())\n",
    "        self.A0 = nn.Parameter(torch.zeros(layer.in_features, self.r) * std_dev)\n",
    "        self.B0 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A1 = nn.Parameter(torch.randn(layer.in_features, self.r) * std_dev)\n",
    "        self.B1 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A2 = nn.Parameter(torch.randn(layer.in_features, self.r) * std_dev)\n",
    "        self.B2 = nn.Parameter(torch.zeros(r, layer.out_features))\n",
    "        self.A = self.A1\n",
    "        self.B = self.B1\n",
    "        self.A0.requires_grad = False\n",
    "        self.B0.requires_grad = False\n",
    "        self.A1.requires_grad = True\n",
    "        self.B1.requires_grad = True\n",
    "        self.A2.requires_grad = False\n",
    "        self.B2.requires_grad = False\n",
    "        \n",
    "    def switch_task(self, task_id):\n",
    "        if task_id == 0:\n",
    "            self.A = self.A0\n",
    "            self.B = self.B0\n",
    "        elif task_id == 1:\n",
    "            self.A = self.A1\n",
    "            self.B = self.B1\n",
    "            self.A1.requires_grad = True\n",
    "            self.B1.requires_grad = True\n",
    "            self.A2.requires_grad = False\n",
    "            self.B2.requires_grad = False\n",
    "        elif task_id == 2:  # Corrected to task_id == 2\n",
    "            self.A = self.A2\n",
    "            self.B = self.B2\n",
    "            self.A1.requires_grad = False\n",
    "            self.B1.requires_grad = False\n",
    "            self.A2.requires_grad = True\n",
    "            self.B2.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.layer(x) + self.alpha * (x @ self.A @ self.B)\n",
    "        return result\n",
    "\n",
    "class BertWithSwitchableTask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertWithSwitchableTask, self).__init__()\n",
    "        self.bert = copy.deepcopy(bert)\n",
    "        self.lora_layers = []\n",
    "        self.add_lora_layers()\n",
    "\n",
    "    def switch_task(self, task_id):\n",
    "        for layer in self.lora_layers:\n",
    "            layer.switch_task(task_id)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        result = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return result\n",
    "\n",
    "    def add_lora_layers(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for layer in self.bert.distilbert.transformer.layer:\n",
    "                layer.attention.q_lin = LoRALayer(layer.attention.q_lin)\n",
    "                layer.attention.k_lin = LoRALayer(layer.attention.k_lin)\n",
    "                layer.attention.v_lin = LoRALayer(layer.attention.v_lin)\n",
    "                layer.attention.out_lin = LoRALayer(layer.attention.out_lin)\n",
    "                layer.ffn.lin1 = LoRALayer(layer.ffn.lin1)\n",
    "                layer.ffn.lin2 = LoRALayer(layer.ffn.lin2)\n",
    "                self.lora_layers += [\n",
    "                    layer.attention.q_lin,\n",
    "                    layer.attention.k_lin,\n",
    "                    layer.attention.v_lin,\n",
    "                    layer.attention.out_lin,\n",
    "                    layer.ffn.lin1,\n",
    "                    layer.ffn.lin2,\n",
    "                ]\n",
    "        self.bert.pre_classifier = LoRALayer(self.bert.pre_classifier)\n",
    "        self.bert.classifier = LoRALayer(self.bert.classifier)\n",
    "        self.lora_layers += [\n",
    "            self.bert.pre_classifier,\n",
    "            self.bert.classifier,\n",
    "        ]\n",
    "        \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.bert.parameters() if p.requires_grad)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:34:13.900924Z",
     "start_time": "2024-06-25T22:34:13.771976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"TRAINABLE PARAMS FOR MODEL: {Bert().count_trainable_parameters()}\")\n",
    "print(f\"TRAINABLE PARAMS FOR LORA MODEL: {BertWithSwitchableTask().count_trainable_parameters()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINABLE PARAMS FOR MODEL: 66958086\n",
      "TRAINABLE PARAMS FOR LORA MODEL: 170508\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:37:09.551377Z",
     "start_time": "2024-06-25T23:37:09.528327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class CustomDataloader:\n",
    "    def __init__(self, name, label, prct):\n",
    "        self.label = label\n",
    "        \n",
    "        # Load the dataset\n",
    "        news_dataset = load_dataset(name, split=f\"train[:{prct}%]\")\n",
    "        news_dataset = news_dataset.train_test_split(test_size=0.5)  # Split the dataset into train and test sets\n",
    "            \n",
    "        train_dataset = news_dataset['train']\n",
    "        test_dataset = news_dataset['test']\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label])\n",
    "        test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label])\n",
    "        \n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "        \n",
    "    def train(self):\n",
    "        return self.train_dataloader\n",
    "        \n",
    "    def test(self):\n",
    "        return self.test_dataloader"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:34:13.918730Z",
     "start_time": "2024-06-25T22:34:13.909263Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "import os\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\"ag_news\"\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, dataloader: CustomDataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def train_model(self, model, name):\n",
    "        model.to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        num_epochs = 5\n",
    "        optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            i = 0\n",
    "            progress_bar = tqdm.tqdm(self.dataloader.train(), desc=f\"Training\")\n",
    "            for batch in progress_bar:\n",
    "                i += 1\n",
    "                optimizer.zero_grad()\n",
    "                inputs = {\"input_ids\": batch[\"input_ids\"].to(device), \"attention_mask\": batch[\"attention_mask\"].to(device), \"labels\": batch[self.dataloader.label].to(device)}\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs.logits, batch[self.dataloader.label].to(device))\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progress_bar.set_postfix(epoch=epoch, loss=train_loss/(i+1))\n",
    "            scheduler.step()\n",
    "            print(f\"Eval accuracy = {self.eval_model(model)}\")\n",
    "            \n",
    "        \n",
    "        torch.save(model, f\"./models/{name}.pt\")\n",
    "        return model\n",
    "        \n",
    "    def eval_model(self, model):\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.dataloader.test():\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch[self.dataloader.label].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_predictions += len(labels)\n",
    "        \n",
    "        eval_accuracy = correct_predictions / total_predictions\n",
    "        return eval_accuracy"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:34:19.361157Z",
     "start_time": "2024-06-25T22:34:13.920130Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader_news = CustomDataloader(\"ag_news\", \"label\", 8)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4800 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae58783f09c04deab1d51a7829b8d06f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4800 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c76b60580974495b8abd0898d78f25b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:59:50.631929Z",
     "start_time": "2024-06-25T23:59:46.209503Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader_trec = CustomDataloader(\"trec\", \"coarse_label\", 100)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2726 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b88d9d7975bc48a69e0397a8beebfe85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2726 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fc420642707455184113fe36fc33ed4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:34:22.846544Z",
     "start_time": "2024-06-25T22:34:22.732875Z"
    }
   },
   "source": "bert_switchable = BertWithSwitchableTask().to(device)",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:43:01.906578Z",
     "start_time": "2024-06-25T22:34:22.848189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(1)\n",
    "bert_switchable = Trainer(dataloader_trec).train_model(bert_switchable, \"distilbert_lora\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:12<00:00, 18.85it/s, epoch=0, loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8950843727072634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:22<00:00, 16.56it/s, epoch=1, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8980190755685987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:21<00:00, 16.65it/s, epoch=2, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.921863536316948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:18<00:00, 17.43it/s, epoch=3, loss=0.114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.9310344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:15<00:00, 18.09it/s, epoch=4, loss=0.0766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.9328686720469552\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:57:22.448335Z",
     "start_time": "2024-06-25T22:43:01.907996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(2)\n",
    "bert_switchable = Trainer(dataloader_news).train_model(bert_switchable, \"distilbert_lora\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:03<00:00, 19.41it/s, epoch=0, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8727083333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:04<00:00, 19.24it/s, epoch=1, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8945833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:16<00:00, 17.64it/s, epoch=2, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8929166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:05<00:00, 19.12it/s, epoch=3, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.90125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:05<00:00, 19.09it/s, epoch=4, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.908125\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:06:48.820439Z",
     "start_time": "2024-06-25T22:57:22.450795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_original_trec = Bert()\n",
    "bert_original_trec = Trainer(dataloader_trec).train_model(bert_original_trec, \"distilbert_original_trec\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:32<00:00, 14.73it/s, epoch=0, loss=0.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8345561261922231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:32<00:00, 14.71it/s, epoch=1, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8910491562729274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:33<00:00, 14.53it/s, epoch=2, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8782098312545855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:31<00:00, 14.87it/s, epoch=3, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8961848862802642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1363/1363 [01:30<00:00, 15.01it/s, epoch=4, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.9104915627292737\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:23:04.061600Z",
     "start_time": "2024-06-25T23:06:48.822008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_original_news = Bert()\n",
    "bert_original_news = Trainer(dataloader_news).train_model(bert_original_news, \"distilbert_original_news\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:38<00:00, 15.11it/s, epoch=0, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.6625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:32<00:00, 15.78it/s, epoch=1, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.6404166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:38<00:00, 15.12it/s, epoch=2, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8585416666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:33<00:00, 15.66it/s, epoch=3, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8654166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2400/2400 [02:49<00:00, 14.12it/s, epoch=4, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy = 0.8635416666666667\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:23:04.357773Z",
     "start_time": "2024-06-25T23:23:04.063151Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(bert, f\"./models/distilbert_original.pt\")",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:27:32.092401Z",
     "start_time": "2024-06-25T23:23:04.359210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_switchable.switch_task(0)\n",
    "print(f\"Accuracy on news dataset without switching: {Trainer(dataloader_news).eval_model(bert_switchable)}\")\n",
    "print(f\"Accuracy on trec dataset without switching: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "\n",
    "bert_switchable.switch_task(1)\n",
    "print(f\"Accuracy on ag_news dataset when switched to trec: {Trainer(dataloader_news).eval_model(bert_switchable)}\")\n",
    "bert_switchable.switch_task(2)\n",
    "print(f\"Accuracy on trec dataset when switched to ag_news: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "\n",
    "bert_switchable.switch_task(1)\n",
    "print(f\"Accuracy on trec dataset when switched to trec: {Trainer(dataloader_trec).eval_model(bert_switchable)}\")\n",
    "bert_switchable.switch_task(2)\n",
    "print(f\"Accuracy on news dataset when switched to news: {Trainer(dataloader_news).eval_model(bert_switchable)}\")\n",
    "\n",
    "print(f\"Accuracy of fine-tuned bert on news dataset: {Trainer(dataloader_news).eval_model(bert_original_news)}\")\n",
    "print(f\"Accuracy of fine-tuned bert on trec dataset: {Trainer(dataloader_trec).eval_model(bert_original_trec)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on news dataset without switching: 0.211875\n",
      "Accuracy on trec dataset without switching: 0.05062362435803375\n",
      "Accuracy on ag_news dataset when switched to trec: 0.15375\n",
      "Accuracy on trec dataset when switched to ag_news: 0.10051357300073367\n",
      "Accuracy on trec dataset when switched to trec: 0.9328686720469552\n",
      "Accuracy on news dataset when switched to news: 0.908125\n",
      "Accuracy of fine-tuned bert on news dataset: 0.8635416666666667\n",
      "Accuracy of fine-tuned bert on trec dataset: 0.9104915627292737\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T00:33:36.368740Z",
     "start_time": "2024-06-26T00:33:35.265979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "bert_switchable.switch_task(1)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to switch task: {end - start}\")\n",
    "\n",
    "start = time.process_time()\n",
    "test = torch.load(\"models/distilbert_original.pt\").to(device)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to load model: {end - start}\")\n",
    "\n",
    "start = time.process_time()\n",
    "test = torch.load(\"models/distilbert_lora.pt\").to(device)\n",
    "bert_switchable.switch_task(1)\n",
    "bert_switchable.switch_task(0)\n",
    "bert_switchable.switch_task(1)\n",
    "bert_switchable.switch_task(2)\n",
    "bert_switchable.switch_task(1)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to load lora model from drive and then switch tasks 5 times: {end - start}\")\n",
    "\n",
    "start = time.process_time()\n",
    "test = torch.load(\"models/distilbert_original.pt\").to(device)\n",
    "test = torch.load(\"models/distilbert_original_trec.pt\").to(device)\n",
    "test = torch.load(\"models/distilbert_original.pt\").to(device)\n",
    "test = torch.load(\"models/distilbert_original_trec.pt\").to(device)\n",
    "test = torch.load(\"models/distilbert_original_news.pt\").to(device)\n",
    "test = torch.load(\"models/distilbert_original_trec.pt\").to(device)\n",
    "end = time.process_time()\n",
    "print(f\"Time it takes to load 6 different models from drive: {end - start}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to switch task: 0.000921797999581031\n",
      "Time it takes to load model: 0.14060739699925762\n",
      "Time it takes to load lora model from drive and then switch tasks 5 times: 0.1611400900001172\n",
      "Time it takes to load 6 different models from drive: 0.7930469319999247\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T23:27:32.950193Z",
     "start_time": "2024-06-25T23:27:32.947377Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
