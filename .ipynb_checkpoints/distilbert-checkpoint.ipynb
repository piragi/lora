{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980550fa-e685-481a-a474-8c1a03257956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using distilbert to finetune on squad, doing different versions of r and a full finetune\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57857632-9115-4d13-b629-7f1bd47383dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, weight, bias, r, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = bias\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        out_features = self.weight.shape[0]\n",
    "        in_features = self.weight.shape[1]\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, in_features))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(out_features, r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = x @ self.weight.T\n",
    "        result = torch.add(result, self.bias)\n",
    "        result = torch.add(result, x @ (self.A.T @ self.B.T))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ce389-20df-4609-8aab-4a9f1b2964b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all the attention layers in model with LoRA layers\n",
    "r = 4\n",
    "alpha = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear) and \"_lin\" in str(name):\n",
    "        lora_layer = LoRALayer(module.weight, module.bias, r, alpha)\n",
    "        # Replace the module directly in the parent's _modules dictionary\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = dict(model.named_modules())[parent_name]\n",
    "        parent_module._modules[child_name] = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084fed9-b689-48a4-a576-1ba2493749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRALayer):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"_lin\" in name: assert param.requires_grad == True\n",
    "    else: assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2e2cc-fc69-4244-b0a8-e07e77e0b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\")\n",
    "\n",
    "# Function to tokenize the data suitable for question answering\n",
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # We need to find where the answers are in the tokenized context\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        # We assume that each question has exactly one answer\n",
    "        start_char = examples[\"answers\"][i][\"answer_start\"][0]\n",
    "        end_char = start_char + len(examples[\"answers\"][i][\"text\"][0]) - 1\n",
    "        \n",
    "        # Convert character start and end positions to token start and end positions\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        # Find start and end token index for the answers\n",
    "        start_index = next(\n",
    "            (idx for idx, (offset, seq_id) in enumerate(zip(offsets, sequence_ids)) if seq_id == 1 and offset[0] <= start_char < offset[1]),\n",
    "            None\n",
    "        )\n",
    "        end_index = next(\n",
    "            (idx for idx, (offset, seq_id) in enumerate(zip(offsets, sequence_ids)) if seq_id == 1 and offset[0] < end_char <= offset[1]),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "    \n",
    "    # Update tokenized examples with the start and end positions\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "# Apply the function to the train dataset\n",
    "train_dataset = dataset['train'].map(prepare_train_features, batched=True)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "val_dataset = dataset['validation'].map(prepare_train_features, batched=True)\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a9eba-f2d6-4e71-a336-f3d2ea1e9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "decoded_text = tokenizer.decode(example['input_ids'])\n",
    "print(decoded_text)\n",
    "answer_tokens = example['input_ids'][example['start_positions']:example['end_positions']+1]\n",
    "decoded_answer = tokenizer.decode(answer_tokens)\n",
    "print(decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b90c67-7a28-40c5-97bf-9fc60a83d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def train(epochs, model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix({'Training Loss': loss.item()})\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Average Training Loss for Epoch {epoch+1}: {avg_loss}\")\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    num_eval_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            num_eval_batches += 1\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / num_eval_batches\n",
    "    print(f\"Average Validation Loss: {avg_eval_loss}\")\n",
    "\n",
    "train(5, model, optimizer, train_loader)\n",
    "eval(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
