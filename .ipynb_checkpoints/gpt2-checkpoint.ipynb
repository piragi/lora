{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, weight, r, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        out_features = self.weight.shape[0]\n",
    "        in_features = self.weight.shape[1]\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, in_features))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(out_features, r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = x @ self.weight.T\n",
    "        result += x @ (self.A.T @ self.B.T)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(FFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e11914972/dl-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.27170369029045105\n",
      "Epoch 20, Loss: 0.24714870750904083\n",
      "Epoch 30, Loss: 0.22865456342697144\n",
      "Epoch 40, Loss: 0.0805884301662445\n",
      "Epoch 50, Loss: 0.16461710631847382\n",
      "Epoch 60, Loss: 0.12893199920654297\n",
      "Epoch 70, Loss: 0.1552349179983139\n",
      "Epoch 80, Loss: 0.10050231963396072\n",
      "Epoch 90, Loss: 0.022541837766766548\n",
      "Epoch 100, Loss: 0.040064822882413864\n",
      "Epoch 110, Loss: 0.01124963816255331\n",
      "Epoch 120, Loss: 0.017011670395731926\n",
      "Epoch 130, Loss: 0.005818037781864405\n",
      "Epoch 140, Loss: 0.011968064121901989\n",
      "Epoch 150, Loss: 0.005529030226171017\n",
      "Epoch 160, Loss: 0.007970748469233513\n",
      "Epoch 170, Loss: 0.002493655076250434\n",
      "Epoch 180, Loss: 0.0033821426331996918\n",
      "Epoch 190, Loss: 0.0031717221718281507\n",
      "Epoch 200, Loss: 0.0015826133312657475\n",
      "Epoch 210, Loss: 0.002305325586348772\n",
      "Epoch 220, Loss: 0.0033826909493654966\n",
      "Epoch 230, Loss: 0.0030965874902904034\n",
      "Epoch 240, Loss: 0.0009779471438378096\n",
      "Epoch 250, Loss: 0.0014687471557408571\n",
      "Epoch 260, Loss: 0.0022783263120800257\n",
      "Epoch 270, Loss: 0.0007349586230702698\n",
      "Epoch 280, Loss: 0.0011209615040570498\n",
      "Epoch 290, Loss: 0.0006177094182930887\n",
      "Epoch 300, Loss: 0.0015673039015382528\n",
      "Epoch 310, Loss: 0.0014461175305768847\n",
      "Epoch 320, Loss: 0.0007539457292295992\n",
      "Epoch 330, Loss: 0.000691656197886914\n",
      "Epoch 340, Loss: 0.0006462950841523707\n",
      "Epoch 350, Loss: 0.0010845158249139786\n",
      "Epoch 360, Loss: 0.0005653291009366512\n",
      "Epoch 370, Loss: 0.0005797764752060175\n",
      "Epoch 380, Loss: 0.0004961424856446683\n",
      "Epoch 390, Loss: 0.0003052442625630647\n",
      "Epoch 400, Loss: 0.00028610514709725976\n",
      "Input: [[0. 0.]], Predicted: [[0.01689894]]\n",
      "Input: [[0. 1.]], Predicted: [[0.9792462]]\n",
      "Input: [[1. 0.]], Predicted: [[0.9779925]]\n",
      "Input: [[1. 1.]], Predicted: [[0.0280364]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "ffn = FFN(2, 16, 1)\n",
    "x_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_xor)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "def train_xor_model(model, dataloader):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(400):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "def validate_xor_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed for predictions\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            print(f\"Input: {inputs.numpy()}, Predicted: {outputs.numpy()}\")\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0014894329942762852\n",
      "Epoch 20, Loss: 0.02709232084453106\n",
      "Epoch 30, Loss: 0.013034832663834095\n",
      "Epoch 40, Loss: 0.6437981128692627\n",
      "Epoch 50, Loss: 0.24719078838825226\n",
      "Epoch 60, Loss: 0.09623385965824127\n",
      "Epoch 70, Loss: 5.1869765371748144e-08\n",
      "Epoch 80, Loss: 3.4163608830795056e-08\n",
      "Epoch 90, Loss: 0.17678774893283844\n",
      "Epoch 100, Loss: 0.023231452330946922\n",
      "Epoch 110, Loss: 0.00125793123152107\n",
      "Epoch 120, Loss: 0.01699119806289673\n",
      "Epoch 130, Loss: 1.4324768926599063e-08\n",
      "Epoch 140, Loss: 0.09364911168813705\n",
      "Epoch 150, Loss: 0.001319304807111621\n",
      "Epoch 160, Loss: 0.0013102362863719463\n",
      "Epoch 170, Loss: 0.06639643013477325\n",
      "Epoch 180, Loss: 0.008611533790826797\n",
      "Epoch 190, Loss: 8.036295184865594e-09\n",
      "Epoch 200, Loss: 0.048913225531578064\n",
      "Epoch 210, Loss: 0.044607315212488174\n",
      "Epoch 220, Loss: 6.436508215301728e-09\n",
      "Epoch 230, Loss: 0.037336550652980804\n",
      "Epoch 240, Loss: 0.03434690460562706\n",
      "Epoch 250, Loss: 5.235975208961463e-09\n",
      "Epoch 260, Loss: 0.029291078448295593\n",
      "Epoch 270, Loss: 4.649564289138652e-09\n",
      "Epoch 280, Loss: 0.0038258146960288286\n",
      "Epoch 290, Loss: 4.113246632186929e-09\n",
      "Epoch 300, Loss: 0.021959058940410614\n",
      "Epoch 310, Loss: 0.0031569090206176043\n",
      "Epoch 320, Loss: 0.0029685331974178553\n",
      "Epoch 330, Loss: 0.002787047065794468\n",
      "Epoch 340, Loss: 0.0006330323521979153\n",
      "Epoch 350, Loss: 0.002481456845998764\n",
      "Epoch 360, Loss: 2.8905020599268028e-09\n",
      "Epoch 370, Loss: 0.014273093082010746\n",
      "Epoch 380, Loss: 0.013523591682314873\n",
      "Epoch 390, Loss: 2.5307258511020336e-09\n",
      "Epoch 400, Loss: 2.4356836547667626e-09\n",
      "Input: [[0. 1.]], Predicted: [[0.97801936]]\n",
      "Input: [[0. 0.]], Predicted: [[0.11004598]]\n",
      "Input: [[1. 1.]], Predicted: [[0.95640904]]\n",
      "Input: [[1. 0.]], Predicted: [[0.99995077]]\n"
     ]
    }
   ],
   "source": [
    "ffn_weight = ffn.linear1.weight.detach().clone()\n",
    "lora_layer = LoRALayer(ffn_weight, 1, 0.1)\n",
    "setattr(ffn, 'linear1', lora_layer)\n",
    "\n",
    "y_or = torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)\n",
    "\n",
    "dataset_xor = TensorDataset(x_xor, y_or)\n",
    "dataloader_xor = DataLoader(dataset_xor, batch_size=1, shuffle=True)\n",
    "\n",
    "train_xor_model(ffn, dataloader_xor)\n",
    "validate_xor_model(ffn, dataloader_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of FFN(\n",
       "  (linear1): LoRALayer()\n",
       "  (linear2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "class LoRAConv1D(nn.Module):\n",
    "    def __init__(self, weight, bias, r, alpha):\n",
    "        super(LoRAConv1D, self).__init__()\n",
    "        self.nx, self.nf = weight.shape\n",
    "        self.weight = weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = bias\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.A = nn.Parameter(self.weight.new_zeros(self.r, self.nx))\n",
    "        self.B = nn.Parameter(self.weight.new_zeros(self.nf, self.r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        low_rank = self.B @ self.A\n",
    "        result += x.view(-1, x.size(-1)) @ low_rank.T\n",
    "        result = result.view(size_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all the attention layers in model with LoRA layers\n",
    "r = 2\n",
    "alpha = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, Conv1D) and \"c_attn\" in str(name):\n",
    "        lora_layer = LoRAConv1D(module.weight, module.bias, r, alpha)\n",
    "        # Replace the module directly in the parent's _modules dictionary\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = dict(model.named_modules())[parent_name]\n",
    "        parent_module._modules[child_name] = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): LoRAConv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv1D):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"attn.c_attn\" in name: assert param.requires_grad == True\n",
    "    else: assert param.requires_grad == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e11914972/dl-env/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/6885 [00:47<?, ?it/s]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     49\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/dl-env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:447\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/dl-env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:337\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    334\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    335\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 337\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m~/dl-env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:259\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "texts = dataset['train']['text']  # Using a small slice for quick training\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).half()\n",
    "\n",
    "# Prepare data for training\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader)*3)\n",
    "\n",
    "# Setup for mixed-precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "progress_bar = tqdm.tqdm(range(len(dataloader) * 5), desc=\"Training\")\n",
    "for epoch in range(5):  # 3 epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        #scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"./model-r{r}-16b-256.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = torch.load(\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sentence = \"Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar.\"\n",
    "inputs = tokenizer(start_sentence, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "model.eval()\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],  # Input tokens\n",
    "    attention_mask=inputs['attention_mask'],  # Attention masks\n",
    "    max_new_tokens=250,  # Specifies the maximum length of the sequence to be generated\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    "    temperature=1.0,  # Sampling temperature\n",
    "    no_repeat_ngram_size=2,  # Prevents repeating n-grams\n",
    "    top_p=0.92,  # Nucleus sampling\n",
    "    top_k=0  # Top-k sampling\n",
    ")\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar. It is a popular beverage in China, and is often used as a tea in Chinese tea shops.\n",
      "\n",
      "The Camella sinus is the most common plant in Asia, with a total of 1,000 species. The plant is found in many\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar. It is also used as a flavoring in tea. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setup device\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnvidia-smi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  9 13:40:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   61C    P0              33W /  70W |    669MiB / 15360MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.50M/3.50M [00:00<00:00, 13.8MB/s]\n",
      "Downloading data: 100%|██████████| 408k/408k [00:00<00:00, 2.52MB/s]\n",
      "Downloading data: 100%|██████████| 76.5k/76.5k [00:00<00:00, 517kB/s]\n",
      "Downloading data: 100%|██████████| 47.5M/47.5M [00:00<00:00, 110MB/s] \n",
      "Generating test split: 100%|██████████| 14042/14042 [00:00<00:00, 768671.99 examples/s]\n",
      "Generating validation split: 100%|██████████| 1531/1531 [00:00<00:00, 557759.00 examples/s]\n",
      "Generating dev split: 100%|██████████| 285/285 [00:00<00:00, 191015.76 examples/s]\n",
      "Generating auxiliary_train split: 100%|██████████| 99842/99842 [00:00<00:00, 386056.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "dataset = load_dataset(\"cais/mmlu\")\n",
    "choices = ['A', 'B', 'C', 'D']\n",
    "\n",
    "def format_subject(subject):\n",
    "    return ' '.join(subject.split('_'))\n",
    "\n",
    "def format_example(df, idx, choices, include_answer=True):\n",
    "    prompt = df['question'][idx]\n",
    "    for j, choice in enumerate(choices):\n",
    "        prompt += f\"\\n{j+1}. {df[choice][idx]}\"\n",
    "    if include_answer:\n",
    "        prompt += f\"\\nAnswer: {df['correct_answer'][idx]}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(df, subject, n_examples=-1):\n",
    "    subject_formatted = format_subject(subject)\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {subject_formatted}.\\n\\n\"\n",
    "    max_examples = df.shape[0] if n_examples == -1 else n_examples\n",
    "    for i in range(max_examples):\n",
    "        prompt += format_example(df, i, ['choice1', 'choice2', 'choice3', 'choice4'], include_answer=True)\n",
    "    return prompt\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dev_df, test_df, subject, num_train_examples):\n",
    "    cors = []\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        train_prompt = gen_prompt(dev_df, subject, num_train_examples)\n",
    "        prompt_end = format_example(test_df, i, ['choice1', 'choice2', 'choice3', 'choice4'], include_answer=False)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        \n",
    "        # Tokenize and ensure input length is within the model's limits\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "        \n",
    "        # Generate logits\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        probs = (\n",
    "            F.softmax(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        logits[tokenizer(\"A\").input_ids[0]],\n",
    "                        logits[tokenizer(\"B\").input_ids[0]],\n",
    "                        logits[tokenizer(\"C\").input_ids[0]],\n",
    "                        logits[tokenizer(\"D\").input_ids[0]],\n",
    "                    ]\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "        pred = np.argmax(probs)\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        correct = test_df['correct_answer'][i]\n",
    "        cor = pred == correct\n",
    "        cors.append(cor)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    accuracy = np.mean(cors)\n",
    "    print(f\"Average accuracy: {accuracy:.3f} - Subject: {subject}\")\n",
    "\n",
    "    return np.array(cors), accuracy, np.array(all_probs)\n",
    "\n",
    "# Example usage\n",
    "dev_df = dataset['validation']\n",
    "test_df = dataset['test']\n",
    "subject = 'subject_name_here'  # Replace with actual subject\n",
    "num_train_examples = 5  # Number of training examples to include in each prompt\n",
    "\n",
    "# Evaluate model\n",
    "results = evaluate(model, tokenizer, dev_df, test_df, subject, num_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
